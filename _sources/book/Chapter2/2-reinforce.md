# REINFORCE

Policy gradient theoremì„ ë‹¤ì‹œ í•œë²ˆ ì ì–´ë³´ì.

$$\nabla_{\theta} J(\theta) \propto \mathbb{E}_{\pi_{\theta}} \left[ Q^{\pi_{\theta}}(s, a) \cdot \nabla_{\theta} \log \pi_{\theta}(a|s) \right]. $$

<br>

ì˜ˆìƒí–ˆê² ì§€ë§Œ, ìœ„ ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ì •í™•íˆ êµ¬í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ê¸°ëŒ“ê°’ì´ì•¼ Monte Carlo ê¸°ë²•ìœ¼ë¡œ ê·¼ì‚¬ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ì •ì±… $\pi_{\theta}$ë¡œ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©ì„ ì—„ì²­ë‚˜ê²Œ ë§ì´ í•´ì„œ trajectory $\tau = (s_0, a_0, r_0, s_1, a_1, r_1,\ldots, s_T)$ë¥¼ ë§ì´ ìƒì„±í•œë‹¤. ê·¸ë¦¬ê³  trajectoryì— ìˆëŠ” ê° $(s_t, a_t)$ë§ˆë‹¤ $Q^{\pi_{\theta}}(s_t, a_t) \cdot \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)$ì„ ê³„ì‚°í•´ì„œ í‘œë³¸ í‰ê· ì„ êµ¬í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. $\nabla_{\theta} \log \pi_{\theta}(a_t|s_t)$ ê³„ì‚°ì€ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì—ì„œ ì•Œì•„ì„œ í•´ì¤€ë‹¤. ê·¸ëŸ°ë° ì—¬ì „íˆ $Q^{\pi_{\theta}}(s_t, a_t)$ë¥¼ ê³„ì‚°í•˜ê¸°ê°€ ì–´ë µë‹¤. ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” $Q^{\pi_{\theta}}(s_t, a_t)$ ëŒ€ì‹  ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ ëŒ€ì²´í•´ì•¼ í•œë‹¤. ì´ $Q^{\pi_{\theta}}(s_t, a_t)$ì„ ì–´ë–¤ ê²ƒìœ¼ë¡œ ëŒ€ì²´í•˜ëŠëƒì— ë”°ë¼ì„œ ì•Œê³ ë¦¬ì¦˜ì˜ ì´ë¦„ì´ ë‹¬ë¼ì§€ê³  ì„±ëŠ¥ë„ í¬ê²Œ ë‹¬ë¼ì§„ë‹¤. ì´ë²ˆ ì¥ì—ì„œëŠ” ê°€ì¥ ì‰½ê³  ê°„ë‹¨í•œ ë°©ë²•ì¸ REINFORCE ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.

<br>

---

## REINFORCE: $Q^{\pi_{\theta}}(s, a)$ ëŒ€ì‹  return ì‚¬ìš©

ì œëª©ì´ ê³§ ë‚´ìš©ì´ë‹¤. REINFORCEëŠ” í•œ ì—í”¼ì†Œë“œë¥¼ ì§„í–‰í•˜ì—¬ í•˜ë‚˜ì˜ trajectory $\tau = (s_0, a_0, r_0, s_1, a_1, r_1,\ldots, s_T)$ë¥¼ ìƒì„±í•˜ê³ , ë‹¤ìŒì˜ policy gradientì˜ ì¶”ì •ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²½ì‚¬í•˜ê°•ë²•ì„ ì§„í–‰í•œë‹¤.
ì•„ë˜ ì‹ì—ì„œ ì‹¤ì œ ê·¸ë ˆë””ì–¸íŠ¸ $\nabla_{\theta} J(\theta)$ì˜ ì¶”ì •ì¹˜ë¥¼ $\hat{g}$ë¡œ í‘œì‹œí•˜ì˜€ë‹¤. 

$$
\nabla_{\theta} J(\theta) \approx \hat{g} := \frac{1}{T} \sum\limits_{t=0}^{T-1} G_t \nabla_{\theta}\log\pi_{\theta}(a_t | s_t),
$$ (reinforce-policy-gradient)

ì´ë•Œ, $G_t = r_t + \gamma r_{t+1} + \ldots + \gamma^{T-t-1} r_{T-1}$ì´ë‹¤. REINFORCEì˜ ì•Œê³ ë¦¬ì¦˜ ë‹¤ìŒê³¼ ê°™ë‹¤.

```{prf:algorithm} REINFORCE
:label: REINFORCE

1. ì •ì±… ë„¤íŠ¸ì›Œí¬ $\pi_{\theta}$ì˜ íŒŒë¼ë¯¸í„° $\theta$ ì´ˆê¸°í™”
2. for _ in range(n_episodes):
3. $\qquad$ $\pi_{\theta}$ë¥¼ ë”°ë¼ ì—í”¼ì†Œë“œë¥¼ ì§„í–‰í•˜ì—¬ $\tau = (s_0, a_0, r_0, s_1, a_1, r_1,\ldots, s_T)$ ìˆ˜ì§‘
4. $\qquad$ for $t=0, 1, 2, \ldots, T-1$:
5. $\qquad\qquad$ $G_t = r_t + \gamma r_{t+1} + \ldots + \gamma^{T-t-1} r_{T-1}$
6. $\qquad\qquad$ $\hat{g}_t = G_t \nabla_{\theta} \log \pi_{\theta} (a_t|s_t)$
7. $\qquad$ $\hat{g} = \frac{1}{T}\sum\limits_{t=0}^{T-1}\hat{g}_{t}$
8. $\qquad$ $\theta \leftarrow \theta + \eta \hat{g}$  $\quad$ `# Gradient ascent`
```

<br>

í•„ìëŠ” ì‹¬ì¸µê°•í™”í•™ìŠµì„ ì±…ìœ¼ë¡œ ê³µë¶€í•˜ë©´ì„œ {prf:ref}`REINFORCE`ì˜ $\nabla_{\theta} \log \pi_{\theta} (a_t|s_t)$ì´ ì–´ë–»ê²Œ ì½”ë“œë¡œ êµ¬í˜„ë˜ëŠ”ì§€ ì •ë§ ê¶ê¸ˆí–ˆë‹¤. ë‹¤ìŒ ì¥ì—ì„œ ì´ì‚° í–‰ë™ ê³µê°„ê³¼ ì—°ì† í–‰ë™ ê³µê°„ì—ì„œ ì–´ë–»ê²Œ REINFORCEê°€ ì–´ë–»ê²Œ êµ¬í˜„ë˜ëŠ”ì§€ ì‹¤ìŠµì„ í†µí•´ ì•Œì•„ë³´ì.

<br>

```{raw} html
<script
   type="text/javascript"
   src="https://utteranc.es/client.js"
   async="async"
   repo="HiddenBeginner/Deep-Reinforcement-Learnings"
   issue-term="pathname"
   theme="github-light"
   label="ğŸ’¬ comment"
   crossorigin="anonymous"
/>
```