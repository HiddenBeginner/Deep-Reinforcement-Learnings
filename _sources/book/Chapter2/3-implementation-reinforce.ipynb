{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE 구현\n",
    "\n",
    "## `Gymnaisum` 사용법\n",
    "\n",
    "먼저 강화학습 코딩이 처음인 분들에게는 에이전트가 환경과 상호작용하는 것부터 어떻게 구현해야 할지 막막할 것이다. `Gymnaisum`은 다양한 연습용 환경을 제공하고 있으며, 강화학습 연구자들에게 환경을 구현하기 위한 일종의 약속 (convention)을 제공하고 있다. 이번 절에서 `Gymnaisum`에서 제공하고 있는 CartPole 환경을 사용해볼 것이다.\n",
    "\n",
    "<br>\n",
    "\n",
    "먼저 실습에 사용할 라이브러리들을 모두 임포트 하자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "`gymnasium.make('환경이름')`을 통해 환경을 불러올 수 있다. `Gymnasium`에서 제공하는 환경의 목록은 `Gymnasium`의 [공식 문서](https://gymnasium.farama.org/environments/classic_control/)에 잘 나와 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "환경 초기화는 `env.reset()` 메서드를 사용하여 할 수 있다. 초기 상태 $s_0 \\sim \\rho_0$에 대응하는 코드이다. 이 메서드를 호출하면 환경의 초기 상태와 주로 학습에 사용되지는 않지만 사용자가 알면 좋은 추가 정보를 반환해준다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state is:  [ 0.04483376 -0.00665065 -0.0137544  -0.02688403]\n",
      "Information:  {}\n"
     ]
    }
   ],
   "source": [
    "s, info = env.reset()\n",
    "\n",
    "print(\"Initial state is: \", s)\n",
    "print(\"Information: \", info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "환경의 상태를 관찰하였으니, 아무 행동을 뽑아서 환경에 행동을 취해보자. `env.action_space.sample()`은 행동 공간에서 임의의 행동 하나를 반환해주는 메서드이다. 다음으로 `env.step(action)`을 통해 환경에 행동을 취할 수 있다. `env.step(action)`은 5-tuple인 `(s_prime, r, terminated, truncated, info)`을 반환해준다.\n",
    "- `s_prime`: 다음 상태\n",
    "- `r`: 보상\n",
    "- `terminated`: 환경이 종료 조건에 의해 종료되었는지 여부\n",
    "- `truncated`: 환경이 최대 상호작용 횟수에 도달하여 종료되었는지 여부\n",
    "- `info`: 추가 정보\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next state:  [ 0.04470074 -0.20157267 -0.01429208  0.2614277 ]\n",
      "Reward:  1.0\n",
      "Is terminated?  False\n",
      "Is truncated?  False\n",
      "Information:  {}\n"
     ]
    }
   ],
   "source": [
    "# Choose a random action\n",
    "a = env.action_space.sample()  \n",
    "\n",
    "# Take the action\n",
    "s_prime, r, terminated, truncated, info =  env.step(a)\n",
    "\n",
    "print(\"Next state: \", s_prime)\n",
    "print(\"Reward: \", r)\n",
    "print(\"Is terminated? \", terminated)\n",
    "print(\"Is truncated? \", truncated)\n",
    "print(\"Information: \", info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "`terminated`와 `truncated`은 둘 다 환경이 종료되었는지 여부를 반환해준다. 단, `terminated`은 종료 조건에 의해 환경이 종료되었을 때 `True`를 반환한다. 종료 조건의 예시는 주로 에이전트가 목적을 달성했거나, 사망이나 붕괴 등 돌이킬 수 없는 상태에 빠졌을 경우를 의미한다. `truncated`은 주로 환경의 종료 조건이 따로 없는 경우에만 의미가 있다. 로봇 통제와 같은 무한히 상호작용할 수 있는 환경의 경우, 실제 구현에서는 평생 상호작용할 수 없으니 1000회 정도 상호작용 후 환경을 마친다. 이처럼 최대 상호작용 횟수에 도달하여 환경이 종료된 경우 `truncated`와 `True`를 반환한다 \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "위의 내용을 종합하면, 한 에피소드 진행은 다음과 같이 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return:  29.0\n"
     ]
    }
   ],
   "source": [
    "s, terminated, truncated, ret = env.reset(), False, False, 0\n",
    "while not (terminated or truncated):\n",
    "    a = env.action_space.sample()\n",
    "    s_prime, r, terminated, truncated, _ = env.step(a)\n",
    "    \n",
    "    ret += r\n",
    "    s = s_prime\n",
    "print(\"Return: \", ret)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## 이산 행동 공간\n",
    "\n",
    "다음으로 정책 네트워크를 정의해보자. 정책은 주어진 상태 $s$에서 행동 $a$를 취한 조건부 확률 $\\text{Pr}\\left[A=a|S=s \\right]$이다. \n",
    "각 상태 $s$마다 조건부 확률 분포가 정의되어야 하므로 정책 네트워크는 상태를 입력 받는다. 그리고 행동에 대한 확률 분포이기 때문에 각 행동에 확률을 부여해야 한다.\n",
    "\n",
    "CartPole 환경은 4차원 행동을 입력 받고 2가지 행동 중 선택하는 이산 행동 공간을 가지기 때문에 4차원 벡터를 입력 받고 2차원 벡터를 출력하는 다층퍼셉트론 (MLP)로 정의하되, 출력이 확률 분포로 해석할 수 있도록 출력에 softmax 함수를 씌울 것이다. 이 MLP 나중에 다양한 실험을 위해서 다소 일반적으로 코드를 짰다. 일반적이라는 의미는 사용자의 수요에 맞게 히든레이어 개수 및 노드 수를 조정할 수 있고 활성화 함수도 설정할 수 있도록 만들었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDiscretePolicy(nn.Module):\n",
    "    def __init__(self, dim_state, dim_action, dim_hiddens=(512, ), activation_fn=F.relu):\n",
    "        super(MLPDiscretePolicy, self).__init__()\n",
    "        self.input_layer = nn.Linear(dim_state, dim_hiddens[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(dim_hiddens) - 1):\n",
    "            hidden_layer = nn.Linear(dim_hiddens[i], dim_hiddens[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(dim_hiddens[-1], dim_action)\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = self.activation_fn(self.input_layer(s))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            s = self.activation_fn(hidden_layer(s))\n",
    "        prob = F.softmax(self.output_layer(s), dim=-1)\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "정책 네트워크를 다음과 같이 선언할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPDiscretePolicy(\n",
       "  (input_layer): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "dim_state = env.observation_space.shape[0]\n",
    "dim_action = env.action_space.n\n",
    "dim_hiddens = (128, 128)\n",
    "activation_fn = F.relu\n",
    "\n",
    "policy = MLPDiscretePolicy(dim_state, dim_action, dim_hiddens, activation_fn)\n",
    "policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "정책 네트워크에 상태 하나를 입력하면 다음과 같이 출력된다. 결과로 나온 벡터의 각 원소는 각 행동을 취할 확률로 해석할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4787, 0.5213], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s, _ = env.reset()\n",
    "s = torch.as_tensor(s, dtype=torch.float)\n",
    "\n",
    "policy(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "정책 네트워크로 만든 확률 분포에서 행동을 샘플링은 다음과 같이 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected action :  1\n"
     ]
    }
   ],
   "source": [
    "prob = policy(s)\n",
    "a = torch.multinomial(prob, num_samples=1)\n",
    "print(\"Selected action : \", a.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "정책 네트워크가 준비되었으니 REINFORCE 에이전트를 만들어보자. REINFORCE 에이전트는 총 4개 메서드가 있다. \n",
    "- `__init__()`: 에이전트 클래스가 입력 받을 하이퍼파라미터를 입력 받고 여러가지 초기화를 수행한다.\n",
    "- `act()`: 환경과의 상호작용을 위한 메서드로서 상태 하나를 입력 받아 행동을 출력한다.\n",
    "- `learn()`: 에이전트가 갖고 있는 네트워크들을 업데이트해준다.\n",
    "- `process()`: 환경과의 매 상호작용마다 수행할 메서드이다. 주로 transition을 버퍼에 저장하고, 특정 주기로 `learn()` 메서드를 호출한다.\n",
    "\n",
    "이 메서드 구성은 카카오 엔터프라이즈의 강화학습 라이브러리 [JORLDY](https://github.com/kakaoenterprise/JORLDY)를 참고하여 만들었다. 이후 구현할 알고리즘들도 동일한 메서드 구성을 갖고 있다. 알고리즘마다 각 메서드가 어떻게 구현되는지에 초점을 맞춰 비교하면 좋을 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(self, policy, gamma=0.99, lr=0.001):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.buffer = []\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def act(self, s, training=True):\n",
    "        self.policy.train(training)\n",
    "\n",
    "        s = torch.as_tensor(s, dtype=torch.float, device=self.device)\n",
    "        prob = self.policy(s)\n",
    "        a = torch.multinomial(prob, 1) if training else torch.argmax(prob, dim=-1, keepdim=True)\n",
    "\n",
    "        return a.cpu().numpy()\n",
    "\n",
    "    def learn(self):\n",
    "        self.policy.train()\n",
    "        s, a, r, _, _, _ = map(np.stack, zip(*self.buffer))\n",
    "        s, a, r = map(lambda x: torch.as_tensor(x, dtype=torch.float, device=self.device), [s, a, r])\n",
    "        a = a.long()\n",
    "        r = r.unsqueeze(1)\n",
    "        \n",
    "        ret = torch.clone(r)\n",
    "        for t in reversed(range(len(ret) - 1)):\n",
    "            ret[t] += self.gamma * ret[t + 1]\n",
    "            \n",
    "        probs = self.policy(s)\n",
    "        log_probs = torch.log(probs.gather(1, a.long()))\n",
    "        \n",
    "        policy_loss = - (ret * log_probs).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        result = {'policy_loss': policy_loss.item()}\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def process(self, transition):\n",
    "        result = None\n",
    "        self.buffer.append(transition)\n",
    "        if transition[-1] or transition[-2]:\n",
    "            result = self.learn()\n",
    "            self.buffer = []\n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "REINFORCE 클래스 작성을 마쳤으니 이제 환경과 상호작용을 하면서 정책 네트워크를 훈련시킬 차례이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env_name, agent, eval_iterations):\n",
    "    env = gym.make(env_name)\n",
    "    scores = []\n",
    "    for _ in range(eval_iterations):\n",
    "        (s, _), terminated, truncated, score = env.reset(seed=np.random.randint(10000)), False, False, 0\n",
    "        while not (terminated or truncated):\n",
    "            a = agent.act(s, training=False)\n",
    "            s_prime, r, terminated, truncated, _ = env.step(a[0])\n",
    "            score += r\n",
    "            s = s_prime\n",
    "        scores.append(score)\n",
    "    env.close()\n",
    "    return round(np.mean(scores), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Steps  10000] Avg return: 477.5\n",
      "[Steps  20000] Avg return: 263.2\n",
      "[Steps  30000] Avg return: 333.7\n",
      "[Steps  40000] Avg return: 500.0\n",
      "[Steps  50000] Avg return: 500.0\n",
      "[Steps  60000] Avg return: 500.0\n",
      "[Steps  70000] Avg return: 500.0\n",
      "[Steps  80000] Avg return: 500.0\n",
      "[Steps  90000] Avg return: 153.1\n",
      "[Steps 100000] Avg return: 500.0\n"
     ]
    }
   ],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "\n",
    "seed = 0\n",
    "max_iterations = 100000\n",
    "eval_intervals = 10000\n",
    "eval_iterations = 10\n",
    "gamma = 0.99\n",
    "lr = 0.001\n",
    "\n",
    "env = gym.make(env_name)\n",
    "policy = MLPDiscretePolicy(dim_state, dim_action)\n",
    "agent = REINFORCE(policy, gamma, lr)\n",
    "\n",
    "(s, _), terminated, truncated = env.reset(), False, False\n",
    "for t in range(1, max_iterations + 1):\n",
    "    a = agent.act(s)\n",
    "    s_prime, r, terminated, truncated, _ = env.step(a[0])\n",
    "    result = agent.process((s, a, r, s_prime, terminated, truncated))\n",
    "    s = s_prime\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        (s, _), terminated, truncated = env.reset(), False, False\n",
    "        \n",
    "    if t % eval_intervals == 0:\n",
    "        score = evaluate(env_name, agent, eval_iterations)\n",
    "        print(f\"[Steps {t:6}] Avg return: {score:.5}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## 연속 행동 공간\n",
    "\n",
    "Coming soon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "853a8469be9b81a4c0bdfe7bce7a8f62601695039c80d5ed818e03c9802f08a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
