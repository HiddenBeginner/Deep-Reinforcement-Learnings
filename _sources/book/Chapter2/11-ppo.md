# Proximal Policy Optimization (PPO)

ì§€ë‚œ ì¥ì—ì„œ ë‹¤ë£¨ì—ˆë˜ TRPOì˜ í•µì‹¬ì€ ì •ì±…ì˜ monotonic improvementê°€ ë³´ì¥ë˜ëŠ” ì˜ì—­ ë‚´ì—ì„œ ì •ì±… ë„¤íŠ¸ì›Œí¬ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤ëŠ” ì ì´ë‹¤.
ë§ì€ ì–´ë µì§€ë§Œ, í˜„ì¬ ì •ì±…ê³¼ "ê°€ê¹Œìš´ ì •ì±…ë“¤ ì¤‘ì—ì„œ" performance measureë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ì •ì±…ì„ ì°¾ëŠ” ê²ƒìœ¼ë¡œ êµ¬í˜„ëœë‹¤.
TRPOëŠ” ëª©ì í•¨ìˆ˜ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ (ê·¸ë ˆë””ì–¸íŠ¸ ë°©í–¥)ìœ¼ë¡œ í˜„ì¬ ì •ì±…ê³¼ì˜ KL divergenceê°€ $\delta$ë³´ë‹¤ ì‘ì•„ì§ˆ ë•Œê¹Œì§€ backtrack line searchë¥¼ í•˜ë©° ì¡°ê±´ì„ ë§Œì¡±ì‹œí‚¤ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•˜ë‹¤.
PPOëŠ” íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±ì‹œí‚¤ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ê¸° ë³´ë‹¤ëŠ” ê·¸ëƒ¥ ì• ì´ˆì— TRPOì˜ ì—…ë°ì´íŠ¸ í¬ê¸°ë¥¼ clipí•˜ì—¬ ì •ì±…ì„ ì¡°ê¸ˆì”©ë§Œ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ë²•ì´ë¼ê³  ìš”ì•½í•  ìˆ˜ ìˆë‹¤.

---

## TRPOì˜ ëª©ì í•¨ìˆ˜ ë³µìŠµ
TRPOëŠ” ì—…ë°ì´íŠ¸ ì „ ì •ì±… $\pi_{\theta_{\text{old}}}$ì™€ ì—…ë°ì´íŠ¸ í›„ ì •ì±… $\pi_{\theta}$ì˜ KL divergenceì— ëŒ€í•œ ì œì•½ (constraint)ì„ ê±¸ì–´ ë‹¤ìŒ  surrogate objectiveë¥¼ ìµœëŒ€í™”ë¥¼ í–ˆë‹¤ (ì°¸ê³ ë¡œ ì‹¤ì œ ìµœì í™”í•˜ê³  ì‹¶ì€ ëª©ì í•¨ìˆ˜ì˜ lower boundë¥¼ ìµœì í™”í•˜ê¸° ë•Œë¬¸ì— surrogate objectiveë¼ê³  ë¶€ë¥¸ë‹¤).

$$
\operatorname*{maximize}_\theta \hat{\mathbb{E}}_t \left[ \frac{\pi_{\theta}\left( a_t |s_t\right)}{\pi_{\theta_{\text{old}}}\left( a_t |s_t\right)} \hat{A}_t\right], \quad \quad (1)
$$

$$
\text{subject to} \quad \hat{\mathbb{E}}_t \left[ \operatorname{KL}\left[ \pi_{\theta_{\text{old}}}\left( \, \cdot \,|s_t \right), \pi_\theta \left( \, \cdot \, | s_t \right) \right] \right] \le \delta. \quad \quad (2)
$$

<br>

í•´ì„í•˜ìë©´, 
- ì •ì±…ì€ ì£¼ì–´ì§„ ìƒíƒœì— ëŒ€í•œ í–‰ë™ë“¤ì˜ í™•ë¥ ë¶„í¬ì´ê¸° ë•Œë¬¸ì— ë‘ ì •ì±… ì‚¬ì´ì˜ KL divergenceë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. 
- ì—…ë°ì´íŠ¸ ì „, í›„ ì •ì±…ì˜ KL divergenceë¥¼ $\delta$ ì´í•˜ë¡œ ìœ ì§€í•˜ë©´ì„œ ì‹ $(1)$ì˜ surrogate objectiveë¥¼ ìµœëŒ€í™”.
- TRPOì—ì„œëŠ” ì‹ $(1), (2)$ì˜ constraint optimization ëŒ€ì‹  ì•„ë˜ì˜ $(3)$ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©ë²•ë„ ì œì•ˆí–ˆì§€ë§Œ, $\beta$ ê°’ì„ í•˜ë‚˜ë¡œ ì •í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì–´ë µë‹¤ê³  í•œë‹¤. $\beta$ë¥¼ í™˜ê²½ì— ë”°ë¼ ì§€ì •í•´ì¤˜ì•¼í•  ë¿ë§Œ ì•„ë‹ˆë¼, ì‚¬ì‹¤ í•™ìŠµ ë„ì¤‘ì—ë„ adaptiveí•˜ê²Œ ë°”ê¿”ì¤˜ì•¼í•  í•„ìš”ê°€ ìˆì—ˆë‹¤.

$$
\operatorname*{maximize}_{\theta} \hat{\mathbb{E}}_t \left[ \frac{\pi_{\theta}\left( a_t |s_t\right)}{\pi_{\theta_{\text{old}}}\left( a_t |s_t\right)} \hat{A}_t -\beta \operatorname{KL}\left[ \pi_{\theta_{\text{old}}}\left( \, \cdot \,|s_t \right), \pi_\theta \left( \, \cdot \, | s_t \right)\right] \right]. \quad \quad (3)
$$

<br>

---

## PPO ì•Œê³ ë¦¬ì¦˜ êµ¬ì„±ìš”ì†Œë“¤
### Clipped surrogate objective

PPOëŠ” KL divergenceë¥¼ ì´ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸ì˜ í¬ê¸°ë¥¼ ì œí•œí•˜ì§€ ì•Šê³ , ì• ì´ˆì— ì—…ë°ì´íŠ¸ ëŒ€ìƒì¸ $\frac{\pi_{\theta}\left( a_t |s_t\right)}{\pi_{\theta_{\text{old}}}\left( a_t |s_t\right)} \hat{A}_t$ì˜ ê°’ì„ clippingí•˜ì—¬ ì œí•œí•˜ëŠ” clipped surrogate objectiveë¥¼ ì‚¬ìš©í•œë‹¤. í‘œê¸°ì˜ í¸ì˜ë¥¼ ìœ„í•´ $r_{t} \left( \theta \right) = \frac{\pi_{\theta} \left( a_t | s_t \right)}{\pi_{\theta_{\text{old}}} \left( a_t | s_t \right)}$ë¼ê³  í•˜ì.

$$
L^{\text{CLIP}}(\theta)=\hat{\mathbb{E}}_t \left[ \min \left(r_t \left(\theta\right)\hat{A}_t, \operatorname{clip}\left(r_t \left(\theta\right), 1-\epsilon,1+\epsilon \right) \hat{A}_t \right) \right], \quad \quad (4)
$$

<br>

where 

$$
\operatorname{clip}\left(x, \text{low}, \text{high} \right)=\begin{cases} \text{low} &  \text{if } x<\text{low}, \\ x & \text{if } \text{low} \le x < \text{high}, \\ \text{high} & \text{if } x \ge \text{high}. \end{cases}
$$

<br>

ìš°ì„  $\theta = \theta_{\text{old}}$ì¼ ë•Œ $r_t \left( \theta \right)=1$ì—ì„œ ì—…ë°ì´íŠ¸ë¥¼ ì‹œì‘í•œë‹¤. ì°¸ê³ ë¡œ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œ ì •ì±… ë„¤íŠ¸ì›Œí¬ì˜ í˜„ì¬ íŒŒë¼ë¯¸í„°ê°€ $\theta_{\text{old}}$ì´ë‹¤. ìˆ˜ì§‘í•œ ë°ì´í„°ë¡œ ì •ì±…ì„ $K$ epochs í›ˆë ¨ì‹œí‚¬ ê²ƒì´ë‹¤. 

<br>

ë§Œì•½ $\hat{A}_t>0$ ë¼ë©´, ìƒíƒœ $s_t$ì—ì„œ í–‰ë™ $a_t$ë¥¼ ì·¨í•  í™•ë¥ ì„ ë†’ì—¬ì£¼ëŠ” ë°©í–¥ìœ¼ë¡œ policyë¥¼ ì—…ë°ì´íŠ¸í•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ $r_t \left( \theta \right)$ì´ 1ë³´ë‹¤ ì»¤ì§€ê²Œ ëœë‹¤. ì´ë•Œ, $\operatorname{clip}$ì€ $r_t \left( \theta \right)$ì´ $1+\epsilon$ ê¹Œì§€ë§Œ ì»¤ì§€ë„ë¡ ë§Œë“¤ì–´ì¤€ë‹¤.

<br>

ë°˜ëŒ€ë¡œ ë§Œì•½ $\hat{A}_t<0$ ë¼ë©´, ìƒíƒœ $s_t$ì—ì„œ í–‰ë™ $a_t$ë¥¼ ì·¨í•  í™•ë¥ ì„ ë‚®ì¶°ì£¼ëŠ” ë°©í–¥ìœ¼ë¡œ policyë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤. ì¦‰, $r_t \left( \theta \right)$ì´ 1ë³´ë‹¤ ì‘ì•„ì§€ê²Œ ëœë‹¤. ì´ë•Œ, $\operatorname{clip}$ì€ $r_t \left( \theta \right)$ì´ $1-\epsilon$ ê¹Œì§€ë§Œ ì‘ì•„ì§€ê²Œ ë§Œë“¤ì–´ì¤€ë‹¤.

```{figure} https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-9-25-ppo/clip.png
---
width: 500px
---
```

<br>

### ìƒíƒœ ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ ë° ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ë¥¼ í¬í•¨í•œ ìµœì¢… ëª©ì  í•¨ìˆ˜

Clipped surrogate objectiveëŠ” ì •ì±… ë„¤íŠ¸ì›Œí¬ë¥¼ ìœ„í•œ ëª©ì  í•¨ìˆ˜ì´ë‹¤. ê·¸ë¦¬ê³  ê·¸ ì•ˆì— ìˆëŠ” $\hat{A}_t$ë¥¼ ê²°ì •í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆìœ¼ë©°, ëŒ€ë¶€ë¶„ ìƒíƒœ ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ë¥¼ í•„ìš”ë¡œ í•œë‹¤. ìš°ë¦¬ê°€ ê³µë¶€í–ˆë˜ $n$-step returnê³¼ GAE ì¤‘ ì•„ë¬´ê±°ë‚˜ ì‚¬ìš©í•´ë„ ëœë‹¤. ë…¼ë¬¸ì—ì„œ ì†Œê°œí•˜ëŠ” advantageì— ëŒ€í•œ ì¶”ì •ëŸ‰ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\hat{A}_{t} = \delta_{t} + (\gamma\lambda)\delta_{t+1}+\cdots+(\gamma\lambda)^{T-t+1}\delta_{T-1}, \quad \quad (5)
$$

$$
\text{where} \quad \delta_t=r_t+\gamma V(s_{t+1}) - V(s_t). \quad \quad (6)
$$

<br>

$\lambda=1$ì¼ ë•Œë¥¼ ì‚´í´ë³´ë©´ ì¡°ê¸ˆ ì™€ë‹¿ëŠ”ë‹¤.

$$
\hat{A}_t = -V(s_t) + r_t + \gamma r_{t+1} + \cdots + \gamma^{T- t +1}r_{T-1}+\gamma^{T-t} V(S_T)
$$


<br>

ìš°ë¦¬ê°€ ì•„ëŠ” advantage $A_t = Q(s_t, a_t) - V(s_t)$ê³¼ ì‹ $(6)$ì„ ë¹„êµí•´ë³´ë©´ $-V(s_t)$ëŠ” ë™ì¼í•˜ê²Œ ê°–ê³  ìˆìœ¼ë©°, ë‚˜ë¨¸ì§€ í…€ $r_{t} + \gamma r_{t+1} + \cdots + \gamma^{T-t+1}r_{T-1} + \gamma^{T-t} V(S_T)$ëŠ” $Q(s_t, a_t)$ë¥¼ ì¶”ì •ëŸ‰ì´ë‹¤. ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” ì „ì²´ ëª©ì  í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
L_{t}^{\text{CLIP}+\text{VF}+\text{S}}\left( \theta \right) = \hat{\mathbb{E}}_t \left[ L_t^{\text{CLIP}} \left( \theta \right) -c_{1}L_{t}^{\text{VF}}\left( \theta \right) + c_{2} S\left[\pi_{\theta}\right]\left( s_{t} \right)\right], \quad \quad (7)
$$

<br>

ì´ë•Œ $L_t^{\text{VF}} \left( \theta \right) = \left( V_{\theta} \left( s_{t} \right) - V_{t}^{\text{targ}} \right)^{2}$ìœ¼ë¡œ ê°€ì¹˜ í•¨ìˆ˜ approximatorë¥¼ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•œ í…€ì´ë‹¤. $V_{t}^{\text{targ}}$ì€ returnì´ ë  ìˆ˜ë„ ìˆê³  $n$-step returnì´ ë  ìˆ˜ë„ ìˆë‹¤. ì£¼ë¡œ returnì„ ì‚¬ìš©í•œë‹¤. $S\left[\pi_{\theta} \right] \left( s_t \right)$ì€ entropy bonusìœ¼ë¡œì„œ explorationì„ í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” í…€ì´ë‹¤. ì—”íŠ¸ë¡œí”¼ì— ê´€í•œ ë‚´ìš©ì€ ë‹¤ìŒ ì£¼ì œì¸ SACì—ì„œ ë” ìì„¸íˆ ì•Œì•„ë³¼ ì˜ˆì •ì´ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ $c_1, c_2$ëŠ” ê° í…€ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ì´ë‹¤.

<br>

---

### ì•Œê³ ë¦¬ì¦˜

ë…¼ë¬¸ì—ì„œ ì†Œê°œí•˜ëŠ” PPO ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. $N$ê°œì˜ policyê°€ ê°ê° ë³‘ë ¬ì ìœ¼ë¡œ í™˜ê²½ê³¼ $T$ë²ˆ ìƒí˜¸ì‘ìš©í•˜ì—¬ $NT$ê°œì˜ ê²½í—˜ ë°ì´í„° íšë“í•˜ê³ , ì´ ê²½í—˜ ë°ì´í„°ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ëª©ì  í•¨ìˆ˜ ìµœì í™”í•œë‹¤ëŠ” ë‚´ìš©ì´ë‹¤. 

```{figure} https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-9-25-ppo/algo.png
---
width: 600px
---
```

<br>

```{note}
ê³µë¶€ ëª©ì ìœ¼ë¡œëŠ” ë³‘ë ¬ í™˜ê²½ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  $N=1$ì˜ ê²½ìš°ë§Œ êµ¬í˜„í•˜ë©´ ì¢‹ì§€ë§Œ, ì•„ì‰½ê²Œë„ PPO ì•Œê³ ë¦¬ì¦˜ì€ $N=1$ì¼ ë•Œ ê±°ì˜ ì˜ ì‘ë™í•˜ì§€ ì•ŠëŠ”ë‹¤. 1ê°œì˜ ì •ì±…ì´ 1ê°œì˜ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ì—¬ ì–»ì€ $T$ê°œì˜ ë°ì´í„°ê°€ ì„œë¡œ ë„ˆë¬´ correlated ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ë„¤íŠ¸ì›Œí¬ê°€ í•´ë‹¹ ë°ì´í„°ì— ì‰½ê²Œ ê³¼ì í•©ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ë™ì¼í•œ íŒŒë¼ë¯¸í„°ë¥¼ ê°–ëŠ” $N$ê°œì˜ ì •ì±…ìœ¼ë¡œ ì„œë¡œ ë‹¤ë¥´ê²Œ ì´ˆê¸°í™”ëœ $N$ê°œì˜ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ì—¬ ì–»ì€ ë°ì´í„°ë“¤ì€ ìƒëŒ€ì ìœ¼ë¡œ correlatedê°€ ëœ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— PPO ë“± on-policy ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ í–¥ìƒì— ê±°ì˜ í•„ìˆ˜ì ì´ë‹¤.
```

<br>

---

## Experiment
PPO ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì„¸íŒ…ì— ëŒ€í•´ì„œ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ë‹¤.
- HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPendulum, Reacher, Swimmer, Walker2d, all â€œ-v1â€, OpenAI Gym.
- Policy network: a MLP with two hidden layers of 64 units, tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard deviations.
- No parameter sharing between policy and value function
- No entropy bonus
- Train for 1 million timesteps
- $\gamma=0.99, \lambda=0.95$

<br>

```{figure} https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-9-25-ppo/exp.png
---
---
```

<br>

---

ë‹¤ìŒ ì¥ì—ì„œëŠ” PPOë¥¼ ì§ì ‘ êµ¬í˜„í•´ë³´ë„ë¡ í•  ê²ƒì´ë‹¤.

<br>

```{raw} html
<script
   type="text/javascript"
   src="https://utteranc.es/client.js"
   async="async"
   repo="HiddenBeginner/Deep-Reinforcement-Learnings"
   issue-term="pathname"
   theme="github-light"
   label="ğŸ’¬ comment"
   crossorigin="anonymous"
/>
```
