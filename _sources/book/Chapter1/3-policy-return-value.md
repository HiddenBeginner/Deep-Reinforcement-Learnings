# 정책, Return, 가치 함수

## 정책 (policy)

지금까지 우리는 순차적 의사 결정 문제를 MDP로 정의하는 방법에 대해서 알아보았다. 에이전트는 정책 (policy)이라는 것을 통해 매 시점마다 환경의 상태에 알맞은 행동을 취해서 환경을 제어한다. 정책 $\pi$는 각 상태 $s$에서 행동 $a$를 취할 확률을 정의하는 함수이다. 즉, $\pi:\mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ such that $\pi(s, a) = \text{Pr}[a | S=s]$인 함수이다. 조건부 확률 분포임을 잘 나타내기 위하여 $\pi(s, a)$ 대신 $\pi(a|s)$로 표기해준다. 비유하건데, 정책은 각 상태마다 어떤 행동을 취해야 할지 적어놓은 지침서이다.

<br>

정책은 확률적 정책 (stochastic policy)과 결정적 정책 (deterministic policy)로 구분될 수 있다. 앞서 사용한 정의가 확률적 정책이다. 각 상태에서 확률에 따라 행동을 선택하기 때문이다. 결정적 정책은 한 상태에서 취할 행동이 딱 하나로 정해져 있는 정책을 의미한다. 해당 행동을 선택할 확률이 $1$이고, 나머지 행동을 선택할 확률이 $0$인 조건부 확률 분포로 해석할 수 있기 때문에 결정적 정책은 확률적 정책의 특별한 경우이다. 결정적 정책의 경우, 각 상태를 입력 받아 취할 행동을 출력하는 함수로 생각할 수 있기 때문에 $\pi:\mathcal{S} \rightarrow \mathcal{A}$ such that $\pi(s)=a$ 으로 적어준다.

<br>

주어진 환경에서 사용할 수 있는 정책은 굉장히 많다. 정책의 정의상 각 상태마다 취할 각 행동을 취할 확률만 정의되어 있으면 정책이 될 수 있다. 따라서, 정책을 따라서 행동을 취하면 받게 되는 누적 보상이 적은 바보 정책이 있을 수도 있고, 받게 되는 누적 보상이 굉장이 큰 좋은 정책이 있을 수 있다. 에이전트의 목표는 정책을 따랐을 때 받게 되는 누적 보상이 가장 큰 정책을 찾아내는 것이다. 

<br>

이를 위해서는 우리는 먼저 좋은 정책과 나쁜 정책의 기준을 세워야 한다. 정책의 성능은 가치 함수라는 것으로 측정될 수 있다. 가치 함수는 정책을 따랐을 때 받게 되는 return의 기댓값이다. 그럼 먼저 return이 무엇인지 알아보자.

<br>

---

## Return

초기 상태 $s_0$에서 시작하여 정책 $\pi$를 따르며 얻은 trajectory를 $\tau$라 하자.

$$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_T, a_T),$$

이때, 
- 초기 상태는 초기 상태 확률 분포로부터 샘플링되었고 $s_0 \sim \rho_0$,
- $t$ 시점의 행동은 정책을 따라 선택되었으며 $a_t \sim \pi(\cdot| s_t)$,
- $t+1$ 시점의 상태는 전이 확률 분포에 따라 바뀌었고 $s_{t+1} \sim p(\cdot|s_t, a_t)$,
- $t$ 시점의 보상은 보상 함수에 의해 결정되었다 $r_t=r(s_t, a_t)$.

<br>

환경은 특정 종료 조건에 의해 $T$ 시점까지만 진행될 수 있지만, 정해진 종료 조건 없이 무한히 진행될 수도 있다. 이후부터는 보다 더 일반적인 상황인 무한히 진행되는 환경을 고려할 것이다[^infinite-horizon].

$$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots).$$

<br>

Return은 $t$시점부터 받은 보상들의 할인된 누적 합 (discounted cummulative sum)이며, $G_t$라고 표기해준다.

$$G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^2 r_{t+3} + \ldots,$$

<br>

이때, 할인률 $\gamma \in [0, 1]$은 0과 1사이의 값이며, $t$ 시점에 취한 행동 $a_t$을 return을 통해 평가할 때, $t$시점보다 먼 시점에 받은 보상일수록 낮은 가중치를 주는 역할을 한다. $\gamma=0$이면, $t$ 시점에 받은 보상 $r_t$만 고려하며, $\gamma=1$이면 $t$ 시점 이후에 받는 모든 보상의 총합을 고려한다. 

<br>

요컨데, return $G_t$는 $t$시점에 취한 행동을 평가하기 위한 값이라고 할 수 있다. $t$시점의 보상 $r_t$만 고려하지 않고 미래에 받을 보상까지 모두 고려하는 이유는 아무튼 $a_t$가 $s_{t+1}$ 만들어 낸 것이고 이후에 받을 보상들이 어떻게 보면 $a_t$ 덕분에 만들어진 것이기 때문이다.

<br>

하지만, 환경과 정책에 있는 많은 확률적 요소 (randomness)에 의해 하나의 관측값인 $G_t$만으로 행동을 평가하기엔 불확실성이 너무 크다. 따라서 우리는 $G_t$의 기댓값으로 정책과 행동의 좋고 나쁨을 평가할 것이다.

<br>

---

## 가치 함수

Coming soon!


[^infinite-horizon]: 끝이 정해져 있는 환경의 경우 종료 조건에 의해 실제 $T$까지만 진행되었어도, 이후 상태가 행동에 의해 바뀌지 않는 종료 상태 (terminal state)로 유지되고 보상은 0을 받는 상호작용을 하는 것으로 간주하여 무한히 진행되는 것으로 생각할 수 있다.