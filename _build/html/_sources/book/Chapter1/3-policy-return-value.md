# 정책, Return, 가치 함수

## 정책 (policy)

지금까지 우리는 순차적 의사 결정 문제를 MDP로 정의하는 방법에 대해서 알아보았다. 에이전트는 정책 (policy)이라는 것을 통해 매 시점마다 환경의 상태에 알맞은 행동을 취해서 환경을 제어한다. 정책 $\pi$는 각 상태 $s$에서 행동 $a$를 취할 확률을 정의하는 함수이다. 즉, $\pi:\mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ such that $\pi(s, a) = \text{Pr}[a | S=s]$인 함수이다. 조건부 확률 분포임을 잘 나타내기 위하여 $\pi(s, a)$ 대신 $\pi(a|s)$로 표기해준다. 비유하건데, 정책은 각 상태마다 어떤 행동을 취해야 할지 적어놓은 지침서이다.

<br>

정책은 확률적 정책 (stochastic policy)과 결정적 정책 (deterministic policy)로 구분될 수 있다. 앞서 사용한 정의가 확률적 정책이다. 각 상태에서 확률에 따라 행동을 선택하기 때문이다. 결정적 정책은 한 상태에서 취할 행동이 딱 하나로 정해져 있는 정책을 의미한다. 해당 행동을 선택할 확률이 $1$이고, 나머지 행동을 선택할 확률이 $0$인 조건부 확률 분포로 해석할 수 있기 때문에 결정적 정책은 확률적 정책의 특별한 경우이다. 결정적 정책의 경우, 각 상태를 입력 받아 취할 행동을 출력하는 함수로 생각할 수 있기 때문에 $\pi:\mathcal{S} \rightarrow \mathcal{A}$ such that $\pi(s)=a$ 으로 적어준다.

<br>

주어진 환경에서 사용할 수 있는 정책은 굉장히 많다. 정책의 정의상 각 상태마다 취할 각 행동을 취할 확률만 정의되어 있으면 정책이 될 수 있다. 따라서, 정책을 따라서 행동을 취하면 받게 되는 누적 보상이 적은 바보 정책이 있을 수도 있고, 받게 되는 누적 보상이 굉장이 큰 좋은 정책이 있을 수 있다. 에이전트의 목표는 정책을 따랐을 때 받게 되는 누적 보상이 가장 큰 정책을 찾아내는 것이다. 

<br>

이를 위해서는 우리는 먼저 좋은 정책과 나쁜 정책의 기준을 세워야 한다. 강화학습에서 가장 중요한 개념인 가치 함수에 대해 알아보자.

<br>

## 가치 함수