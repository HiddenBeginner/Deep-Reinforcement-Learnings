
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>3. 정책, Return, 가치 함수 &#8212; 심층강화학습</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "HiddenBeginner/Deep-Reinforcement-Learnings");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/HDBG.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="4. 벨만 방정식: 가치 함수의 재귀적 성질" href="4-bellman-equation.html" />
    <link rel="prev" title="2. Markov Decision Process (MDP)" href="2-markov-decision-processes.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-FTQEC31PV8"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-FTQEC31PV8');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/HDBG.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">심층강화학습</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    심층강화학습 (Deep Reinforcement Learnings)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1-sequential-decision-making-problems.html">
   1. 순차적 의사 결정 문제, 에이전트, 환경
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-markov-decision-processes.html">
   2. Markov Decision Process (MDP)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. 정책, Return, 가치 함수
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-bellman-equation.html">
   4. 벨만 방정식: 가치 함수의 재귀적 성질
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-stochastic-approximation.html">
   5. 가치 함수 근사하기: Stochastic approximation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Policy gradient methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2/1-policy-gradient-theorem.html">
   6. Policy Gradient Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2/2-reinforce.html">
   7. REINFORCE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2/3-implementation-reinforce.html">
   8. REINFORCE 구현
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2/4-reinforce-with-baseline.html">
   9. REINFORCE with baseline
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2/5-actor-critic.html">
   10. Actor-critic 방법론
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  참고문헌
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Reference.html">
   참고문헌
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/hiddenbeginner/Deep-Reinforcement-Learnings"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/hiddenbeginner/Deep-Reinforcement-Learnings/issues/new?title=Issue%20on%20page%20%2Fbook/Chapter1/3-policy-return-value.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/book/Chapter1/3-policy-return-value.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy">
   3.1. 정책 (policy)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   3.2. Return
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   3.3. 가치 함수
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#state-value-function">
     3.3.1. 상태 가치 함수 (State value function)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#action-value-function">
     3.3.2. 행동 가치 함수 (Action value function)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#advantage-advantage-function">
     3.3.3. Advantage 함수 (Advantage function)
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>정책, Return, 가치 함수</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy">
   3.1. 정책 (policy)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   3.2. Return
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   3.3. 가치 함수
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#state-value-function">
     3.3.1. 상태 가치 함수 (State value function)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#action-value-function">
     3.3.2. 행동 가치 함수 (Action value function)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#advantage-advantage-function">
     3.3.3. Advantage 함수 (Advantage function)
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="return">
<h1><span class="section-number">3. </span>정책, Return, 가치 함수<a class="headerlink" href="#return" title="Permalink to this headline">#</a></h1>
<section id="policy">
<h2><span class="section-number">3.1. </span>정책 (policy)<a class="headerlink" href="#policy" title="Permalink to this headline">#</a></h2>
<p>지금까지 우리는 순차적 의사 결정 문제를 MDP로 정의하는 방법에 대해서 알아보았다. 에이전트는 정책 (policy)이라는 것을 통해 매 시점마다 환경의 상태에 알맞은 행동을 취해서 환경을 제어한다. 정책 <span class="math notranslate nohighlight">\(\pi\)</span>는 각 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 행동 <span class="math notranslate nohighlight">\(a\)</span>를 취할 확률을 정의하는 함수이다. 즉, <span class="math notranslate nohighlight">\(\pi:\mathcal{S} \times \mathcal{A} \rightarrow [0, 1]\)</span> such that <span class="math notranslate nohighlight">\(\pi(s, a) = \text{Pr}[a | S=s]\)</span>인 함수이다. 조건부 확률 분포임을 잘 나타내기 위하여 <span class="math notranslate nohighlight">\(\pi(s, a)\)</span> 대신 <span class="math notranslate nohighlight">\(\pi(a|s)\)</span>로 표기해준다. 비유하건데, 정책은 각 상태마다 어떤 행동을 취해야 할지 적어놓은 지침서이다.</p>
<br>
<p>정책은 확률적 정책 (stochastic policy)과 결정적 정책 (deterministic policy)로 구분될 수 있다. 앞서 사용한 정의가 확률적 정책이다. 각 상태에서 확률에 따라 행동을 선택하기 때문이다. 결정적 정책은 한 상태에서 취할 행동이 딱 하나로 정해져 있는 정책을 의미한다. 해당 행동을 선택할 확률이 <span class="math notranslate nohighlight">\(1\)</span>이고, 나머지 행동을 선택할 확률이 <span class="math notranslate nohighlight">\(0\)</span>인 조건부 확률 분포로 해석할 수 있기 때문에 결정적 정책은 확률적 정책의 특별한 경우이다. 결정적 정책의 경우, 각 상태를 입력 받아 취할 행동을 출력하는 함수로 생각할 수 있기 때문에 <span class="math notranslate nohighlight">\(\pi:\mathcal{S} \rightarrow \mathcal{A}\)</span> such that <span class="math notranslate nohighlight">\(\pi(s)=a\)</span> 으로 적어준다.</p>
<br>
<p>주어진 환경에서 사용할 수 있는 정책은 굉장히 많다. 정책의 정의상 각 상태마다 취할 각 행동을 취할 확률만 정의되어 있으면 정책이 될 수 있다. 따라서, 정책을 따라서 행동을 취하면 받게 되는 누적 보상이 적은 바보 정책이 있을 수도 있고, 받게 되는 누적 보상이 굉장이 큰 좋은 정책이 있을 수 있다. 에이전트의 목표는 정책을 따랐을 때 받게 되는 누적 보상이 가장 큰 정책을 찾아내는 것이다.</p>
<br>
<p>이를 위해서는 우리는 먼저 좋은 정책과 나쁜 정책의 기준을 세워야 한다. 정책의 성능은 가치 함수라는 것으로 측정될 수 있다. 가치 함수는 정책을 따랐을 때 받게 되는 return의 기댓값이다. 그럼 먼저 return이 무엇인지 알아보자.</p>
<br>
</section>
<hr class="docutils" />
<section id="id1">
<h2><span class="section-number">3.2. </span>Return<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<p>초기 상태 <span class="math notranslate nohighlight">\(s_0\)</span>에서 시작하여 정책 <span class="math notranslate nohighlight">\(\pi\)</span>를 따르며 얻은 trajectory를 <span class="math notranslate nohighlight">\(\tau\)</span>라 하자.</p>
<div class="math notranslate nohighlight">
\[\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_T),\]</div>
<p>이때,</p>
<ul class="simple">
<li><p>초기 상태는 초기 상태 확률 분포로부터 샘플링되었고 <span class="math notranslate nohighlight">\(s_0 \sim d_0\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(t\)</span> 시점의 행동은 정책을 따라 선택되었으며 <span class="math notranslate nohighlight">\(a_t \sim \pi(\cdot| s_t)\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(t+1\)</span> 시점의 상태는 전이 확률 분포에 따라 바뀌었고 <span class="math notranslate nohighlight">\(s_{t+1} \sim p(\cdot|s_t, a_t)\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(t\)</span> 시점의 보상은 보상 함수에 의해 결정되었다 <span class="math notranslate nohighlight">\(r_t=r(s_t, a_t)\)</span>.</p></li>
</ul>
<br>
<p>환경은 특정 종료 조건에 의해 <span class="math notranslate nohighlight">\(T\)</span> 시점까지만 진행될 수 있지만, 정해진 종료 조건 없이 무한히 진행될 수도 있다. 이후부터는 보다 더 일반적인 상황인 무한히 진행되는 환경을 고려할 것이다<a class="footnote-reference brackets" href="#infinite-horizon" id="id2">1</a>.</p>
<div class="math notranslate nohighlight">
\[\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots).\]</div>
<br>
<p>Return은 <span class="math notranslate nohighlight">\(t\)</span>시점부터 받은 보상들의 할인된 누적 합 (discounted cummulative sum)이며, <span class="math notranslate nohighlight">\(G_t\)</span>라고 표기해준다.</p>
<div class="math notranslate nohighlight">
\[G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^2 r_{t+3} + \ldots,\]</div>
<br>
<p>이때, 할인률 <span class="math notranslate nohighlight">\(\gamma \in [0, 1]\)</span>은 0과 1사이의 값이며, <span class="math notranslate nohighlight">\(t\)</span> 시점에 취한 행동 <span class="math notranslate nohighlight">\(a_t\)</span>을 return을 통해 평가할 때, <span class="math notranslate nohighlight">\(t\)</span>시점보다 먼 시점에 받은 보상일수록 낮은 가중치를 주는 역할을 한다. <span class="math notranslate nohighlight">\(\gamma=0\)</span>이면, <span class="math notranslate nohighlight">\(t\)</span> 시점에 받은 보상 <span class="math notranslate nohighlight">\(r_t\)</span>만 고려하며, <span class="math notranslate nohighlight">\(\gamma=1\)</span>이면 <span class="math notranslate nohighlight">\(t\)</span> 시점 이후에 받는 모든 보상의 총합을 고려한다.</p>
<br>
<p>요컨데, return <span class="math notranslate nohighlight">\(G_t\)</span>는 <span class="math notranslate nohighlight">\(t\)</span>시점에 취한 행동을 평가하기 위한 값이라고 할 수 있다. <span class="math notranslate nohighlight">\(t\)</span>시점의 보상 <span class="math notranslate nohighlight">\(r_t\)</span>만 고려하지 않고 미래에 받을 보상까지 모두 고려하는 이유는 아무튼 <span class="math notranslate nohighlight">\(a_t\)</span>가 <span class="math notranslate nohighlight">\(s_{t+1}\)</span> 만들어 낸 것이고 이후에 받을 보상들이 어떻게 보면 <span class="math notranslate nohighlight">\(a_t\)</span> 덕분에 만들어진 것이기 때문이다.</p>
<br>
<p>하지만, 환경과 정책에 있는 많은 확률적 요소 (randomness)에 의해 하나의 관측값인 <span class="math notranslate nohighlight">\(G_t\)</span>만으로 행동을 평가하기엔 불확실성이 너무 크다. 따라서 우리는 <span class="math notranslate nohighlight">\(G_t\)</span>의 기댓값으로 정책과 행동의 좋고 나쁨을 평가할 것이다.</p>
<br>
</section>
<hr class="docutils" />
<section id="id3">
<h2><span class="section-number">3.3. </span>가치 함수<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h2>
<p>자, 이제 강화학습에서 가장 중요한 개념 중 하나인 가치 함수에 대해서 알아보자. 가치 함수는 정책의 좋고 나쁨을 수치적으로 측정할 수 있는 함수이다. 여기서 잠깐. 강화 학습에 어떻게 딥러닝을 적용할 수 있을지 다음 질문에 대한 답변을 각자 한번 생각해보자.</p>
<center><i>"어떤 함수를 뉴럴 네트워크로 학습할 것인가? 그리고 어떤 목적함수를 최적화하여 파라미터를 업데이트할 것인가?"</i></center>
<br>
<p>딥러닝에 익숙한 분들이라면 어쩌면 지금까지 공부한 것만으로도 위 질문에 답변을 냈을지도 모른다. 모든 심층 강화 학습 알고리즘이 이 부류에 속하지는 않지만, 한 가지 답은 다음과 같다.</p>
<center><i>정책을 뉴럴 네트워크로 학습할 것이고, 가치 함수를 최대로 만들어주는 방향으로 파라미터를 업데이트할 것이다.</i></center>
<br>
<p>가치 함수는 심층 강화 학습을 알기 위해 꼭 필요한 개념이니 지금부터 차근 차근 공부해보자. 가치 함수에는 크게 3종류가 있다. 상태 가치 함수, 행동 가치 함수, advantage function.</p>
<br>
<hr class="docutils" />
<section id="state-value-function">
<h3><span class="section-number">3.3.1. </span>상태 가치 함수 (State value function)<a class="headerlink" href="#state-value-function" title="Permalink to this headline">#</a></h3>
<p>한 정책 <span class="math notranslate nohighlight">\(\pi\)</span>의 상태 <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>에서의 상태 가치 함수는 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 정책 <span class="math notranslate nohighlight">\(\pi\)</span>를 따랐을 때 받게 되는 return의 기댓값이다. 수식적으로는 다음과 같다.</p>
<div class="tip admonition">
<p class="admonition-title"><strong>상태 가치 함수 (state value function)</strong></p>
<p>한 정책 <span class="math notranslate nohighlight">\(\pi\)</span>의 상태 <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>의 상태 가치 함수 <span class="math notranslate nohighlight">\(V^{\pi}: \mathcal{S} \rightarrow \mathbb{R}\)</span>는 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 정책 <span class="math notranslate nohighlight">\(\pi\)</span>를 따랐을 때 받게 되는 return의 기댓값으로 정의된다. 즉,</p>
<div class="math notranslate nohighlight" id="equation-state-value-function">
<span class="eqno">(3.1)<a class="headerlink" href="#equation-state-value-function" title="Permalink to this equation">#</a></span>\[V^{\pi}(s) := \mathbb{E}_{\pi} \left[ G_t | S_t = s \right] \quad \forall s \in \mathcal{S}.\]</div>
</div>
<br>
<p>가치 함수를 더 잘 이해할 수 있기 위해 확률론적인 이야기를 조금만 해보자. 식 <a class="reference internal" href="#equation-state-value-function">(3.1)</a>의 기댓값 안에 겉으로 보이는 확률 변수 (random variable)는 <span class="math notranslate nohighlight">\(G_t\)</span> 하나이다. 하지만 사실 이 <span class="math notranslate nohighlight">\(G_t\)</span> 안에 엄청나게 많은 확률 변수들이 있다. 우선, <span class="math notranslate nohighlight">\(G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots\)</span>이고, 각 보상은 <span class="math notranslate nohighlight">\(R_{t'}=r(S_{t'}, A_{t'})\)</span>으로 정의되기 때문에 확률 변수가 <span class="math notranslate nohighlight">\(A_t, S_{t+1}, A_{t+1}, \ldots\)</span>이 있다. <span class="math notranslate nohighlight">\(S_t\)</span>는 조건부에 의해 <span class="math notranslate nohighlight">\(S_t=s\)</span>로 결정되었기 때문에 확률 변수가 아니다.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>본 책에서는 확률 변수 (random variable)는 대문자로, 확률 변수의 실현값 (realization)은 소문자로 표기하려고 최대한 노력하였다.</p>
</div>
<br>
<p>기댓값을 계산할 때는 각 확률 변수가 어떤 확률 분포를 따르는지  <span class="math notranslate nohighlight">\(\mathbb{E}\)</span>의 아래 첨자로 적어줘야 한다.</p>
<ul class="simple">
<li><p>확률 변수 <span class="math notranslate nohighlight">\(A_t\)</span>는 정책에 의해 결정되기 때문에 <span class="math notranslate nohighlight">\(A_t \sim \pi(\cdot|s)\)</span>이고,</p></li>
<li><p>확률 변수 <span class="math notranslate nohighlight">\(S_{t+1}\)</span>은 전이 확률 분포에 의해 결정되기 때문에 <span class="math notranslate nohighlight">\(S_{t+1} \sim p(\cdot\ | s, A_t)\)</span>이며,</p></li>
<li><p>다시 정책을 따라 확률 변수 <span class="math notranslate nohighlight">\(A_{t+1} \sim \pi(\cdot|S_{t+1}), S_{t+2} \sim p(\cdot\ | s_{t+1}, A_{t+1})\)</span></p></li>
<li><p>그리고 이후 모든 확률 변수에 대해서도 적어줘야 한다.</p></li>
</ul>
<br>
<p>하지만 이 모든 확률 변수를 적어줄 수 없기 때문에 <span class="math notranslate nohighlight">\(\mathbb{E}_{\pi}\)</span>라고 적어주었으며, 이 표기에는 다음이 내포되어 있다. 요컨데, 그냥 정책 상태가 <span class="math notranslate nohighlight">\(s\)</span>인 <span class="math notranslate nohighlight">\(t\)</span> 시점부터 정책 <span class="math notranslate nohighlight">\(\pi\)</span>을 쭉 따랐다는 의미이다.</p>
<div class="math notranslate nohighlight">
\[A_{t} \sim \pi(\cdot|s), \; S_{k+1} \sim p(\cdot | S_{k}, A_{k}), \; A_k \sim \pi(\cdot | S_k), \forall k=t+1, t+2, \ldots\]</div>
<br>
<p>정책의 상태 가치 함수의 정의에 대해 알아보았다. 상태 가치 함수를 통해 두 정책 <span class="math notranslate nohighlight">\(\pi\)</span>와 <span class="math notranslate nohighlight">\(\pi'\)</span>의 성능을 비교할 수 있다. 모든 상태 <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>에 대해서 정책 <span class="math notranslate nohighlight">\(\pi\)</span>의 상태 가치 함수가 정책 <span class="math notranslate nohighlight">\(\pi'\)</span>보다 클 때, 우리는 정책 <span class="math notranslate nohighlight">\(\pi\)</span>가 <span class="math notranslate nohighlight">\(\pi'\)</span>가 더 좋다고 말한다. 즉,</p>
<div class="math notranslate nohighlight" id="equation-policy-order">
<span class="eqno">(3.2)<a class="headerlink" href="#equation-policy-order" title="Permalink to this equation">#</a></span>\[\pi \ge \pi', \text{ if } \; V^{\pi}(s) \ge V^{\pi'}(s) \text{ for all } s \in \mathcal{S}.\]</div>
<br>
<p>식 <a class="reference internal" href="#equation-policy-order">(3.2)</a>에 정의된 순서 (order) 사용하여 정책을 쭉 줄세웠을 때, 가장 좋은 정책을 최적의 정책 (optimal policy)라고 부르며 보통 <span class="math notranslate nohighlight">\(\pi^{*}\)</span>라고 부른다.</p>
<br>
<p>그리고 심층 강화 학습에서 정책을 뉴럴 네트워크로 학습시킬 때 사용하는 목적 함수 중 하나는 초기 상태에 대한 가치 함수의 기댓값이다. 즉,</p>
<div class="math notranslate nohighlight">
\[J(\theta) := \mathbb{E}_{S_0 \sim d_0, \pi_\theta} \left[ V^{\pi}(S_0) \right].\]</div>
<br>
</section>
<hr class="docutils" />
<section id="action-value-function">
<h3><span class="section-number">3.3.2. </span>행동 가치 함수 (Action value function)<a class="headerlink" href="#action-value-function" title="Permalink to this headline">#</a></h3>
<p>상태 가치 함수 <span class="math notranslate nohighlight">\(V^{\pi}(s)\)</span>가 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 정책 <span class="math notranslate nohighlight">\(\pi\)</span>의 성능을 알려주는 함수였다면, 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 (정책을 따르지 않고) 행동 <span class="math notranslate nohighlight">\(a\)</span>를 취했을 때 정책 <span class="math notranslate nohighlight">\(\pi\)</span>의 성능을 알려주는 함수를 행동 가치 함수라고 한다.</p>
<div class="tip admonition">
<p class="admonition-title"><strong>행동 가치 함수 (action value function)</strong></p>
<p><span class="math notranslate nohighlight">\(\mathcal{A}\)</span>에서의 행동 가치 함수 <span class="math notranslate nohighlight">\(Q^{\pi}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\)</span>는 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 행동 <span class="math notranslate nohighlight">\(a\)</span>를 취하고 정책 <span class="math notranslate nohighlight">\(\pi\)</span>를 따랐을 때 받게 되는 return의 기댓값으로 정의된다. 즉,</p>
<div class="math notranslate nohighlight" id="equation-action-value-function">
<span class="eqno">(3.3)<a class="headerlink" href="#equation-action-value-function" title="Permalink to this equation">#</a></span>\[Q^{\pi}(s, a) := \mathbb{E}_{\pi} \left[ G_t | S_t = s, A_t=a  \right] \quad \forall s \in \mathcal{S}, a \in \mathcal{A}.\]</div>
</div>
<br>
<p>식 <a class="reference internal" href="#equation-action-value-function">(3.3)</a>에서 조건부의 <span class="math notranslate nohighlight">\(A_t=a\)</span>는 <span class="math notranslate nohighlight">\(t\)</span> 시점의 행동이 <span class="math notranslate nohighlight">\(a\)</span>로 주어졌다는 것을 의미한다. 상태 가치 함수의 정의인 식 <a class="reference internal" href="#equation-state-value-function">(3.1)</a>에서  <span class="math notranslate nohighlight">\(A_t\)</span>가 <span class="math notranslate nohighlight">\(\pi(\cdot | s)\)</span>에서 샘플링된 것과 다르다.</p>
<br>
<p>상태 가치 함수는 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 정책 <span class="math notranslate nohighlight">\(\pi\)</span>를 바로 따랐을 때 받게 되는 return의 기댓값이고, 행동 가치 함수는 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 정해진 행동 <span class="math notranslate nohighlight">\(a\)</span>를 취한 후 <span class="math notranslate nohighlight">\(\pi\)</span>를 따랐을 때 받게 되는 return의 기댓값이다. 그럼 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 다음을 만족하는 행동 <span class="math notranslate nohighlight">\(a\)</span>가 있다는 것은 무엇을 의미할까?</p>
<div class="math notranslate nohighlight">
\[Q^{\pi}(s, a) \ge V^{\pi}(s).\]</div>
<br>
<p>상태 <span class="math notranslate nohighlight">\(s\)</span>에서 정책을 바로 따르는 것보다, 행동 <span class="math notranslate nohighlight">\(a\)</span>를 취하고 정책을 따랐을 때 더 큰 return을 기대할 수 있다는 것이다. 그럼, 우리는 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 행동 <span class="math notranslate nohighlight">\(a\)</span>를 취하도록 정책을 변경함으로서 정책을 개선할 수 있다.</p>
<br>
<p>조금 더 극적으로 정책을 개선할 수도 있다. 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 가장 큰 행동 가치 함수를 갖는 행동을 찾아서 정책을 개선하는 것이다. 즉, 현재 정책을 <span class="math notranslate nohighlight">\(\pi\)</span>, 개선된 정책을  <span class="math notranslate nohighlight">\(\pi'\)</span>라고 하자. 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 가장 큰 <span class="math notranslate nohighlight">\(Q^{\pi}(s, a)\)</span>를 갖는 행동 <span class="math notranslate nohighlight">\(a\)</span>를 1의 확률로 선택하게 정책을 수정하는 것이다.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\pi'(a|s)=\begin{cases} 1 &amp; \text{ if } a = \operatorname*{argmax}\limits_{a\in\mathcal{A}} Q^{\pi}(s, a) \\ 0 &amp; \text{ otherwise} \end{cases}, \text{ for all } s \in \mathcal{S}\end{split}\]</div>
<br>
<p>한편, 알아두면 좋은 두 가치 함수 사이의 관계식이 있다. 기댓값의 정의를 잘 생각해보면, 상태 가치 함수와 행동 가치 함수가 다음과 같은 관계를 갖고 있는 것을 알 수 있다. 증명은 다음 절에 나올 예정이다 (예정일껄…).</p>
<div class="math notranslate nohighlight">
\[V^{\pi}(s) = \mathbb{E}_{a\sim\pi(\cdot|s)} \left[ Q^{\pi}(s, a) \right].\]</div>
<br>
</section>
<hr class="docutils" />
<section id="advantage-advantage-function">
<h3><span class="section-number">3.3.3. </span>Advantage 함수 (Advantage function)<a class="headerlink" href="#advantage-advantage-function" title="Permalink to this headline">#</a></h3>
<p>Advantage 함수는 상태 <span class="math notranslate nohighlight">\(s\)</span>와 행동 <span class="math notranslate nohighlight">\(a\)</span>에 대해서 정의되는 함수로서, 행동 가치 함수에서 상태 가치 함수를 뺀 것이다. 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 정책을 따르는 대신 행동 <span class="math notranslate nohighlight">\(a\)</span>를 취했을 때, 상태 가치 함수와 비교하여 얼마나 더 많은 이득 (advantage)를 얻는가를 나타낸다. Advantage 함수값이 0보다 큰 행동은 현재 정책을 따르는 것보다 더 좋은 행동을 의미하며, 0보다 작은 행동을 현재 정책을 따르는 것보다 더 안 좋은 행동을 의미한다.</p>
<div class="tip admonition">
<p class="admonition-title"><strong>Advantage 함수 (advantage function)</strong></p>
<p>한 정책 <span class="math notranslate nohighlight">\(\pi\)</span>의 상태 <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>와 행동 <span class="math notranslate nohighlight">\(a \in \mathcal{A}\)</span>에서의 advantage 함수 <span class="math notranslate nohighlight">\(A^{\pi}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\)</span>는 행동 가치 함수에서 상태 가치 함수를 뺀 것으로 정의된다. 즉,</p>
<div class="math notranslate nohighlight" id="equation-advantage-function">
<span class="eqno">(3.4)<a class="headerlink" href="#equation-advantage-function" title="Permalink to this equation">#</a></span>\[A^{\pi}(s, a) := Q^{\pi}(s, a) - V^{\pi}(s) \quad \forall s \in \mathcal{S}, a \in \mathcal{A}.\]</div>
</div>
<br>
<hr class="docutils" />
<p>이번 절에서는 세 가지 가치 함수를 알아보았다. 가치 함수만 알면 정책의 성능을 평가할 수 있고, 성능을 최대화하는 정책을 찾으면 이제 게임이 끝난다. 딥러닝에서 목적함수가 준비된 셈이다. 하지만 문제가 하나 있다. 대부분의 경우 이 가치 함수를 실제로 계산하는 것이 불가능하다. 따라서 대부분의 심층 강화 학습 알고리즘들은 가치 함수를 추정하거나 가치 함수의 그레디언트를 추정하게 된다. 이를 추정하기 위한 중요한 개념 하나 남아 있는데, 이를 다음 절에서 알아본다.</p>
<br>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="infinite-horizon"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>끝이 정해져 있는 환경의 경우 종료 조건에 의해 실제 <span class="math notranslate nohighlight">\(T\)</span>까지만 진행되었어도, 이후 상태가 행동에 의해 바뀌지 않는 종료 상태 (terminal state)로 유지되고 보상은 0을 받는 상호작용을 하는 것으로 간주하여 무한히 진행되는 것으로 생각할 수 있다.</p>
</dd>
</dl>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./book/Chapter1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="2-markov-decision-processes.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">2. </span>Markov Decision Process (MDP)</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="4-bellman-equation.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>벨만 방정식: 가치 함수의 재귀적 성질</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 재야의 숨은 초보<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>