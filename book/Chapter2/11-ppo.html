
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>16. Proximal Policy Optimization (PPO) &#8212; 심층강화학습</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "HiddenBeginner/Deep-Reinforcement-Learnings");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/HDBG.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="17. Soft Actor-Critic (SAC)" href="13-sac.html" />
    <link rel="prev" title="15. TRPO 구현" href="10-implementation-trpo.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-FTQEC31PV8"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-FTQEC31PV8');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/HDBG.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">심층강화학습</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    심층강화학습 (Deep Reinforcement Learnings)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/1-sequential-decision-making-problems.html">
   1. 순차적 의사 결정 문제, 에이전트, 환경
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/2-markov-decision-processes.html">
   2. Markov Decision Process (MDP)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/3-policy-return-value.html">
   3. 정책, Return, 가치 함수
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/4-bellman-equation.html">
   4. 벨만 방정식: 가치 함수의 재귀적 성질
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/5-stochastic-approximation.html">
   5. 가치 함수 근사하기: Stochastic approximation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Policy gradient methods
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1-policy-gradient-theorem.html">
   6. Policy Gradient Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-reinforce.html">
   7. REINFORCE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-implementation-reinforce.html">
   8. REINFORCE 구현
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-reinforce-with-baseline.html">
   9. REINFORCE with baseline
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-actor-critic.html">
   10. Actor-critic 알고리즘
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6-implementation-actor-critic.html">
   11. Online/batch actor-critic 구현
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="7-n_step-actor-critic.html">
   12.
   <span class="math notranslate nohighlight">
    \(n\)
   </span>
   -step return actor-critic
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="8-gae.html">
   13. Generalized Advantage Estimation (GAE)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="9-trpo.html">
   14. Trust Region Policy Optimization (TRPO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10-implementation-trpo.html">
   15. TRPO 구현
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   16. Proximal Policy Optimization (PPO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13-sac.html">
   17. Soft Actor-Critic (SAC)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14-implementation-sac.html">
   18. SAC 구현
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  참고문헌
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Reference.html">
   참고문헌
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/hiddenbeginner/Deep-Reinforcement-Learnings"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/hiddenbeginner/Deep-Reinforcement-Learnings/issues/new?title=Issue%20on%20page%20%2Fbook/Chapter2/11-ppo.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/book/Chapter2/11-ppo.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#trpo">
   16.1. TRPO의 목적함수 복습
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ppo">
   16.2. PPO 알고리즘 구성요소들
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clipped-surrogate-objective">
     16.2.1. Clipped surrogate objective
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     16.2.2. 상태 가치 네트워크 및 엔트로피 보너스를 포함한 최종 목적 함수
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     16.2.3. 알고리즘
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experiment">
   16.3. Experiment
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Proximal Policy Optimization (PPO)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#trpo">
   16.1. TRPO의 목적함수 복습
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ppo">
   16.2. PPO 알고리즘 구성요소들
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clipped-surrogate-objective">
     16.2.1. Clipped surrogate objective
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     16.2.2. 상태 가치 네트워크 및 엔트로피 보너스를 포함한 최종 목적 함수
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     16.2.3. 알고리즘
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experiment">
   16.3. Experiment
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="proximal-policy-optimization-ppo">
<h1><span class="section-number">16. </span>Proximal Policy Optimization (PPO)<a class="headerlink" href="#proximal-policy-optimization-ppo" title="Permalink to this headline">#</a></h1>
<p>지난 장에서 다루었던 TRPO의 핵심은 정책의 monotonic improvement가 보장되는 영역 내에서 정책 네트워크의 파라미터를 업데이트한다는 점이다.
말은 어렵지만, 현재 정책과 “가까운 정책들 중에서” performance measure를 증가시키는 정책을 찾는 것으로 구현된다.
TRPO는 목적함수를 최대화하는 방향 (그레디언트 방향)으로 현재 정책과의 KL divergence가 <span class="math notranslate nohighlight">\(\delta\)</span>보다 작아질 때까지 backtrack line search를 하며 조건을 만족시키는 파라미터를 찾았다.
PPO는 특정 조건을 만족시키는 파라미터를 찾기 보다는 그냥 애초에 TRPO의 업데이트 크기를 clip하여 정책을 조금씩만 업데이트 하는 방법이라고 요약할 수 있다.</p>
<hr class="docutils" />
<section id="trpo">
<h2><span class="section-number">16.1. </span>TRPO의 목적함수 복습<a class="headerlink" href="#trpo" title="Permalink to this headline">#</a></h2>
<p>TRPO는 업데이트 전 정책 <span class="math notranslate nohighlight">\(\pi_{\theta_{\text{old}}}\)</span>와 업데이트 후 정책 <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span>의 KL divergence에 대한 제약 (constraint)을 걸어 다음  surrogate objective를 최대화를 했다 (참고로 실제 최적화하고 싶은 목적함수의 lower bound를 최적화하기 때문에 surrogate objective라고 부른다).</p>
<div class="math notranslate nohighlight">
\[
\operatorname*{maximize}_\theta \hat{\mathbb{E}}_t \left[ \frac{\pi_{\theta}\left( a_t |s_t\right)}{\pi_{\theta_{\text{old}}}\left( a_t |s_t\right)} \hat{A}_t\right], \quad \quad (1)
\]</div>
<div class="math notranslate nohighlight">
\[
\text{subject to} \quad \hat{\mathbb{E}}_t \left[ \operatorname{KL}\left[ \pi_{\theta_{\text{old}}}\left( \, \cdot \,|s_t \right), \pi_\theta \left( \, \cdot \, | s_t \right) \right] \right] \le \delta. \quad \quad (2)
\]</div>
<br>
<p>해석하자면,</p>
<ul class="simple">
<li><p>정책은 주어진 상태에 대한 행동들의 확률분포이기 때문에 두 정책 사이의 KL divergence를 계산할 수 있다.</p></li>
<li><p>업데이트 전, 후 정책의 KL divergence를 <span class="math notranslate nohighlight">\(\delta\)</span> 이하로 유지하면서 식 <span class="math notranslate nohighlight">\((1)\)</span>의 surrogate objective를 최대화.</p></li>
<li><p>TRPO에서는 식 <span class="math notranslate nohighlight">\((1), (2)\)</span>의 constraint optimization 대신 아래의 <span class="math notranslate nohighlight">\((3)\)</span>을 최대화하는 방법도 제안했지만, <span class="math notranslate nohighlight">\(\beta\)</span> 값을 하나로 정하는 것이 매우 어렵다고 한다. <span class="math notranslate nohighlight">\(\beta\)</span>를 환경에 따라 지정해줘야할 뿐만 아니라, 사실 학습 도중에도 adaptive하게 바꿔줘야할 필요가 있었다.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\operatorname*{maximize}_{\theta} \hat{\mathbb{E}}_t \left[ \frac{\pi_{\theta}\left( a_t |s_t\right)}{\pi_{\theta_{\text{old}}}\left( a_t |s_t\right)} \hat{A}_t -\beta \operatorname{KL}\left[ \pi_{\theta_{\text{old}}}\left( \, \cdot \,|s_t \right), \pi_\theta \left( \, \cdot \, | s_t \right)\right] \right]. \quad \quad (3)
\]</div>
<br>
</section>
<hr class="docutils" />
<section id="ppo">
<h2><span class="section-number">16.2. </span>PPO 알고리즘 구성요소들<a class="headerlink" href="#ppo" title="Permalink to this headline">#</a></h2>
<section id="clipped-surrogate-objective">
<h3><span class="section-number">16.2.1. </span>Clipped surrogate objective<a class="headerlink" href="#clipped-surrogate-objective" title="Permalink to this headline">#</a></h3>
<p>PPO는 KL divergence를 이용하여 업데이트의 크기를 제한하지 않고, 애초에 업데이트 대상인 <span class="math notranslate nohighlight">\(\frac{\pi_{\theta}\left( a_t |s_t\right)}{\pi_{\theta_{\text{old}}}\left( a_t |s_t\right)} \hat{A}_t\)</span>의 값을 clipping하여 제한하는 clipped surrogate objective를 사용한다. 표기의 편의를 위해 <span class="math notranslate nohighlight">\(r_{t} \left( \theta \right) = \frac{\pi_{\theta} \left( a_t | s_t \right)}{\pi_{\theta_{\text{old}}} \left( a_t | s_t \right)}\)</span>라고 하자.</p>
<div class="math notranslate nohighlight">
\[
L^{\text{CLIP}}(\theta)=\hat{\mathbb{E}}_t \left[ \min \left(r_t \left(\theta\right)\hat{A}_t, \operatorname{clip}\left(r_t \left(\theta\right), 1-\epsilon,1+\epsilon \right) \hat{A}_t \right) \right], \quad \quad (4)
\]</div>
<br>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\operatorname{clip}\left(x, \text{low}, \text{high} \right)=\begin{cases} \text{low} &amp;  \text{if } x&lt;\text{low}, \\ x &amp; \text{if } \text{low} \le x &lt; \text{high}, \\ \text{high} &amp; \text{if } x \ge \text{high}. \end{cases}
\end{split}\]</div>
<br>
<p>우선 <span class="math notranslate nohighlight">\(\theta = \theta_{\text{old}}\)</span>일 때 <span class="math notranslate nohighlight">\(r_t \left( \theta \right)=1\)</span>에서 업데이트를 시작한다. 참고로 환경과 상호작용하여 데이터를 수집한 정책 네트워크의 현재 파라미터가 <span class="math notranslate nohighlight">\(\theta_{\text{old}}\)</span>이다. 수집한 데이터로 정책을 <span class="math notranslate nohighlight">\(K\)</span> epochs 훈련시킬 것이다.</p>
<br>
<p>만약 <span class="math notranslate nohighlight">\(\hat{A}_t&gt;0\)</span> 라면, 상태 <span class="math notranslate nohighlight">\(s_t\)</span>에서 행동 <span class="math notranslate nohighlight">\(a_t\)</span>를 취할 확률을 높여주는 방향으로 policy를 업데이트하게 된다. 따라서 <span class="math notranslate nohighlight">\(r_t \left( \theta \right)\)</span>이 1보다 커지게 된다. 이때, <span class="math notranslate nohighlight">\(\operatorname{clip}\)</span>은 <span class="math notranslate nohighlight">\(r_t \left( \theta \right)\)</span>이 <span class="math notranslate nohighlight">\(1+\epsilon\)</span> 까지만 커지도록 만들어준다.</p>
<br>
<p>반대로 만약 <span class="math notranslate nohighlight">\(\hat{A}_t&lt;0\)</span> 라면, 상태 <span class="math notranslate nohighlight">\(s_t\)</span>에서 행동 <span class="math notranslate nohighlight">\(a_t\)</span>를 취할 확률을 낮춰주는 방향으로 policy를 업데이트한다. 즉, <span class="math notranslate nohighlight">\(r_t \left( \theta \right)\)</span>이 1보다 작아지게 된다. 이때, <span class="math notranslate nohighlight">\(\operatorname{clip}\)</span>은 <span class="math notranslate nohighlight">\(r_t \left( \theta \right)\)</span>이 <span class="math notranslate nohighlight">\(1-\epsilon\)</span> 까지만 작아지게 만들어준다.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-9-25-ppo/clip.png"><img alt="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-9-25-ppo/clip.png" src="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-9-25-ppo/clip.png" style="width: 500px;" /></a>
</figure>
<br>
</section>
<section id="id1">
<h3><span class="section-number">16.2.2. </span>상태 가치 네트워크 및 엔트로피 보너스를 포함한 최종 목적 함수<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<p>Clipped surrogate objective는 정책 네트워크를 위한 목적 함수이다. 그리고 그 안에 있는 <span class="math notranslate nohighlight">\(\hat{A}_t\)</span>를 결정하는 다양한 방법이 있으며, 대부분 상태 가치 네트워크를 필요로 한다. 우리가 공부했던 <span class="math notranslate nohighlight">\(n\)</span>-step return과 GAE 중 아무거나 사용해도 된다. 논문에서 소개하는 advantage에 대한 추정량은 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
\hat{A}_{t} = \delta_{t} + (\gamma\lambda)\delta_{t+1}+\cdots+(\gamma\lambda)^{T-t+1}\delta_{T-1}, \quad \quad (5)
\]</div>
<div class="math notranslate nohighlight">
\[
\text{where} \quad \delta_t=r_t+\gamma V(s_{t+1}) - V(s_t). \quad \quad (6)
\]</div>
<br>
<p><span class="math notranslate nohighlight">\(\lambda=1\)</span>일 때를 살펴보면 조금 와닿는다.</p>
<div class="math notranslate nohighlight">
\[
\hat{A}_t = -V(s_t) + r_t + \gamma r_{t+1} + \cdots + \gamma^{T- t +1}r_{T-1}+\gamma^{T-t} V(S_T)
\]</div>
<br>
<p>우리가 아는 advantage <span class="math notranslate nohighlight">\(A_t = Q(s_t, a_t) - V(s_t)\)</span>과 식 <span class="math notranslate nohighlight">\((6)\)</span>을 비교해보면 <span class="math notranslate nohighlight">\(-V(s_t)\)</span>는 동일하게 갖고 있으며, 나머지 텀 <span class="math notranslate nohighlight">\(r_{t} + \gamma r_{t+1} + \cdots + \gamma^{T-t+1}r_{T-1} + \gamma^{T-t} V(S_T)\)</span>는 <span class="math notranslate nohighlight">\(Q(s_t, a_t)\)</span>를 추정량이다. 이 논문에서 제안하는 전체 목적 함수는 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
L_{t}^{\text{CLIP}+\text{VF}+\text{S}}\left( \theta \right) = \hat{\mathbb{E}}_t \left[ L_t^{\text{CLIP}} \left( \theta \right) -c_{1}L_{t}^{\text{VF}}\left( \theta \right) + c_{2} S\left[\pi_{\theta}\right]\left( s_{t} \right)\right], \quad \quad (7)
\]</div>
<br>
<p>이때 <span class="math notranslate nohighlight">\(L_t^{\text{VF}} \left( \theta \right) = \left( V_{\theta} \left( s_{t} \right) - V_{t}^{\text{targ}} \right)^{2}\)</span>으로 가치 함수 approximator를 훈련시키기 위한 텀이다. <span class="math notranslate nohighlight">\(V_{t}^{\text{targ}}\)</span>은 return이 될 수도 있고 <span class="math notranslate nohighlight">\(n\)</span>-step return이 될 수도 있다. 주로 return을 사용한다. <span class="math notranslate nohighlight">\(S\left[\pi_{\theta} \right] \left( s_t \right)\)</span>은 entropy bonus으로서 exploration을 하게 만들어주는 텀이다. 엔트로피에 관한 내용은 다음 주제인 SAC에서 더 자세히 알아볼 예정이다. 마지막으로 <span class="math notranslate nohighlight">\(c_1, c_2\)</span>는 각 텀에 대한 가중치이다.</p>
<br>
</section>
<hr class="docutils" />
<section id="id2">
<h3><span class="section-number">16.2.3. </span>알고리즘<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h3>
<p>논문에서 소개하는 PPO 알고리즘은 다음과 같다. <span class="math notranslate nohighlight">\(N\)</span>개의 policy가 각각 병렬적으로 환경과 <span class="math notranslate nohighlight">\(T\)</span>번 상호작용하여 <span class="math notranslate nohighlight">\(NT\)</span>개의 경험 데이터 획득하고, 이 경험 데이터들을 사용하여 목적 함수 최적화한다는 내용이다.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-9-25-ppo/algo.png"><img alt="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-9-25-ppo/algo.png" src="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-9-25-ppo/algo.png" style="width: 600px;" /></a>
</figure>
<br>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>공부 목적으로는 병렬 환경을 사용하지 않고 <span class="math notranslate nohighlight">\(N=1\)</span>의 경우만 구현하면 좋지만, 아쉽게도 PPO 알고리즘은 <span class="math notranslate nohighlight">\(N=1\)</span>일 때 거의 잘 작동하지 않는다. 1개의 정책이 1개의 환경과 상호작용하여 얻은 <span class="math notranslate nohighlight">\(T\)</span>개의 데이터가 서로 너무 correlated 되어 있기 때문에 네트워크가 해당 데이터에 쉽게 과적합되기 때문이다. 동일한 파라미터를 갖는 <span class="math notranslate nohighlight">\(N\)</span>개의 정책으로 서로 다르게 초기화된 <span class="math notranslate nohighlight">\(N\)</span>개의 환경과 상호작용하여 얻은 데이터들은 상대적으로 correlated가 덜 되어 있기 때문에 PPO 등 on-policy 알고리즘 성능 향상에 거의 필수적이다.</p>
</div>
<br>
</section>
</section>
<hr class="docutils" />
<section id="experiment">
<h2><span class="section-number">16.3. </span>Experiment<a class="headerlink" href="#experiment" title="Permalink to this headline">#</a></h2>
<p>PPO 논문에서는 다음과 같은 세팅에 대해서 실험을 진행하였다.</p>
<ul class="simple">
<li><p>HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPendulum, Reacher, Swimmer, Walker2d, all “-v1”, OpenAI Gym.</p></li>
<li><p>Policy network: a MLP with two hidden layers of 64 units, tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard deviations.</p></li>
<li><p>No parameter sharing between policy and value function</p></li>
<li><p>No entropy bonus</p></li>
<li><p>Train for 1 million timesteps</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma=0.99, \lambda=0.95\)</span></p></li>
</ul>
<br>
<figure class="align-default">
<img alt="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-9-25-ppo/exp.png" src="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-9-25-ppo/exp.png" />
</figure>
<br>
<hr class="docutils" />
<p>다음 장에서는 PPO를 직접 구현해보도록 할 것이다.</p>
<br>
<script
   type="text/javascript"
   src="https://utteranc.es/client.js"
   async="async"
   repo="HiddenBeginner/Deep-Reinforcement-Learnings"
   issue-term="pathname"
   theme="github-light"
   label="💬 comment"
   crossorigin="anonymous"
/></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./book/Chapter2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="10-implementation-trpo.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">15. </span>TRPO 구현</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="13-sac.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">17. </span>Soft Actor-Critic (SAC)</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 재야의 숨은 초보<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>