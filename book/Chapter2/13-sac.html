
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>16. Soft Actor-Critic (SAC) &#8212; 심층강화학습</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "HiddenBeginner/Deep-Reinforcement-Learnings");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/HDBG.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="참고문헌" href="../Reference.html" />
    <link rel="prev" title="15. TRPO 구현" href="10-implementation-trpo.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-FTQEC31PV8"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-FTQEC31PV8');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/HDBG.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">심층강화학습</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    심층강화학습 (Deep Reinforcement Learnings)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/1-sequential-decision-making-problems.html">
   1. 순차적 의사 결정 문제, 에이전트, 환경
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/2-markov-decision-processes.html">
   2. Markov Decision Process (MDP)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/3-policy-return-value.html">
   3. 정책, Return, 가치 함수
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/4-bellman-equation.html">
   4. 벨만 방정식: 가치 함수의 재귀적 성질
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/5-stochastic-approximation.html">
   5. 가치 함수 근사하기: Stochastic approximation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Policy gradient methods
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1-policy-gradient-theorem.html">
   6. Policy Gradient Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-reinforce.html">
   7. REINFORCE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-implementation-reinforce.html">
   8. REINFORCE 구현
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-reinforce-with-baseline.html">
   9. REINFORCE with baseline
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-actor-critic.html">
   10. Actor-critic 알고리즘
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6-implementation-actor-critic.html">
   11. Online/batch actor-critic 구현
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="7-n_step-actor-critic.html">
   12.
   <span class="math notranslate nohighlight">
    \(n\)
   </span>
   -step return actor-critic
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="8-gae.html">
   13. Generalized Advantage Estimation (GAE)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="9-trpo.html">
   14. Trust Region Policy Optimization (TRPO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10-implementation-trpo.html">
   15. TRPO 구현
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   16. Soft Actor-Critic (SAC)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  참고문헌
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Reference.html">
   참고문헌
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/hiddenbeginner/Deep-Reinforcement-Learnings"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/hiddenbeginner/Deep-Reinforcement-Learnings/issues/new?title=Issue%20on%20page%20%2Fbook/Chapter2/13-sac.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/book/Chapter2/13-sac.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   16.1. 사전 지식
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#on-policy-vs-off-policy">
     16.1.1. On-policy vs. Off-policy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     16.1.2. 엔트로피
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#soft-policy-iteration">
   16.2. Soft policy iteration
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#soft-policy-evaluation">
     16.2.1. Soft policy evaluation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#soft-policy-improvement">
     16.2.2. Soft policy improvement
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#soft-actor-critic">
   16.3. Soft Actor-Critic
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     16.3.1. 준비물
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#soft-state-value-function">
     16.3.2. Soft state value function 학습
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#soft-q-function">
     16.3.3. Soft Q-function 학습
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#policy">
     16.3.4. Policy 학습
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experiment">
   16.4. Experiment
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Soft Actor-Critic (SAC)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   16.1. 사전 지식
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#on-policy-vs-off-policy">
     16.1.1. On-policy vs. Off-policy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     16.1.2. 엔트로피
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#soft-policy-iteration">
   16.2. Soft policy iteration
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#soft-policy-evaluation">
     16.2.1. Soft policy evaluation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#soft-policy-improvement">
     16.2.2. Soft policy improvement
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#soft-actor-critic">
   16.3. Soft Actor-Critic
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     16.3.1. 준비물
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#soft-state-value-function">
     16.3.2. Soft state value function 학습
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#soft-q-function">
     16.3.3. Soft Q-function 학습
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#policy">
     16.3.4. Policy 학습
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experiment">
   16.4. Experiment
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="soft-actor-critic-sac">
<h1><span class="section-number">16. </span>Soft Actor-Critic (SAC)<a class="headerlink" href="#soft-actor-critic-sac" title="Permalink to this headline">#</a></h1>
<p>이번 장에서는 필자가 느끼기에 심층강화학습 분야에서 가장 널리 사용되는 알고리즘인 Soft Actor-Critic (SAC)에 대해 알아볼 예정이다.
2018년에 공개된 SAC을 더 이상 state-of-the-art (SOTA)라고 부르진 않지만, 많은 연구들이 SAC 알고리즘을 베이스라인로 사용하여 발전시키고 있다.
특히, 더 적은 environment steps만으로 더 높은 성능을 달성하고자 하는 sample efficiency 분야에서 SAC가 많이 사용된다. 예를 들어,</p>
<ul class="simple">
<li><p>벡터 상태 공간에서는 SAC <span class="math notranslate nohighlight">\(\rightarrow\)</span> REDQ (2021 ICLR) <span class="math notranslate nohighlight">\(\rightarrow\)</span> DroQ (2022 ICLR) <span class="math notranslate nohighlight">\(\rightarrow\)</span> CrossQ (2024 ICLR)으로 발전되었으며,</p></li>
<li><p>픽셀 상태 공간에서는 SAC <span class="math notranslate nohighlight">\(\rightarrow\)</span> DrQ (2021 ICLR) <span class="math notranslate nohighlight">\(\rightarrow\)</span> Reset (2022 ICML) <span class="math notranslate nohighlight">\(\rightarrow\)</span> PLASTIC (2023 NeurIPS)으로 발전되어 왔다.</p></li>
</ul>
<br>
<p>위 알고리즘들은 필자가 알고 있는 것들만 적어 놓은 것이며, 실제로는 훨씬 더 많은 알고리즘들이 SAC를 기반으로 하고 있다.
SAC이 많이 사용되는 알고리즘이긴 하지만 내용이 쉬운 편은 아니다. 따라서 SAC를 이해하는 데 필요한 사전 지식들부터 알아보자.</p>
<br>
<hr class="docutils" />
<section id="id1">
<h2><span class="section-number">16.1. </span>사전 지식<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<section id="on-policy-vs-off-policy">
<h3><span class="section-number">16.1.1. </span>On-policy vs. Off-policy<a class="headerlink" href="#on-policy-vs-off-policy" title="Permalink to this headline">#</a></h3>
<p>필자가 작성하고 있는 이 책은 어디서부터 어떻게 잘못된 것일까?
어렵고 헷갈리는 용어 사용을 최대한 지양하다보니 on-policy와 off-policy라는 용어를 이제서야 소개를 한다.</p>
<br>
<p><strong>On-policy 기법</strong>은 에이전트가 현재 정책을 기준으로 환경과 직접 상호작용하여 데이터를 수집하고, 그때 그때 정책 및 가치 네트워크를 업데이트해나가는 기법이다.
환경과 한 번 상호작용하여 얻은 데이터 <span class="math notranslate nohighlight">\((s, a, r, s’)\)</span>로 네트워크들을 업데이트할 수도 있고, 특정 횟수만큼 상호작용 후 업데이트할 수도 있다.
여기서 중요한 점은 한 번 업데이트에 사용된 데이터는 버려진다는 것이다. 업데이트된 정책은 더 이상 현재 정책이 아니기 때문이다.
수집한 데이터를 재활용하지 않고 업데이트하는 데 한 번만 사용하기 때문에 <strong>sample inefficient</strong>하다.
지금까지 알아본 REINFORCE, TRPO, PPO이 on-policy 알고리즘에 속한다.</p>
<br>
<p><strong>Off policy 기법</strong>은 환경과 상호작용하면서 데이터를 수집하는 정책 (behavior policy)과 업데이트 대상이 되는 정책 (target policy)이 다른 기법을 말한다.
극단적인 예로는 모든 상태에 대해서 임의의 행동을 취하는 정책으로 데이터를 수집하고, 그 데이터를 사용해서 최적의 정책 (타겟 정책)을 찾아나간다.
Off-policy 기법은 수집한 데이터를 여러번 사용하기 때문에 상대적으로 sample efficient하지만, 좋은 정책을 찾기까지 오랜 시간이 소요된다. SAC은 off-policy 알고리즘에 속한다.</p>
<br>
</section>
<hr class="docutils" />
<section id="id2">
<h3><span class="section-number">16.1.2. </span>엔트로피<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h3>
<p>엔트로피는 확률 변수 (random variable)에 대해 정의되는 어떤 값이다. 잘 와닿지 않는다면 확률 분포에 정의되는 어떤 값이라고 생각해도 좋다.
확률 분포 <span class="math notranslate nohighlight">\(p(x)\)</span>를 갖는 확률 변수 <span class="math notranslate nohighlight">\(X\)</span>의 엔트로피는 다음과 같이 정의된다. 참고로 두 번째 등호는 이산확률변수일 때 기대값의 정의이다.</p>
<div class="math notranslate nohighlight">
\[
\mathcal{H}(X)=\mathbb{E}\left[-\log p(X) \right]=-\sum_{x \in \mathcal{X}} p(x)\log p(x).
\]</div>
<br>
<p>무엇을 의미하는지 전혀 감도 안 오지 않는다. 직관적으로 말하자면 엔트로피는 확률 변수의 <strong>불확실성</strong>이다. 그럼, 확실한 분포는 무엇이고, 불확실한 분포는 무엇일까? 다음 그림을 보자. <a class="reference external" href="https://www.cantorsparadise.com/the-information-gain-of-model-rejection-facb46e8be36">출처</a></p>
<figure class="align-default">
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-11-6-sac/Untitled.png"><img alt="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-11-6-sac/Untitled.png" src="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-11-6-sac/Untitled.png" style="width: 500px;" /></a>
</figure>
<br>
<p>먼저, 가장 오른쪽 그림. 한 값에 확률이 1이고, 나머지에 대해서는 확률이 0인 상황이다. 이 분포를 보면 <span class="math notranslate nohighlight">\(X\)</span>를 어떤 것이라고 예상할 수 있을까?
“<span class="math notranslate nohighlight">\(X\)</span>는 항상 2다.” 라고 확실하게 말할 수 있을 것이다.
이런 분포는 확실한 분포이고, 그래서 불확실성을 나타내는 엔트로피 값은 0이다.
두 번째로 가장 왼쪽 그림은 균등 분포이다. 이 분포를 보면 <span class="math notranslate nohighlight">\(X\)</span>가 어떤 것이라고 예상할 수 있는가?
<span class="math notranslate nohighlight">\(X\)</span>가 무엇이라고 특정지을 수 없을 것이다. 그래서 불확실성이 제일 높다. 불확실성이 높을수록 균등분포에 가깝다.</p>
<br>
<p>강화학습에서는 Maximization Entropy 기법이 많이 등장한다.
어떤 매개변수로 표현되는 분포 <span class="math notranslate nohighlight">\(p_\theta\)</span>를 찾는데, 분포가 큰 엔트로피 값을 갖도록 매개변수 <span class="math notranslate nohighlight">\(\theta\)</span>를 찾는 기법이다.
불확실성이 큰 것이 나쁜 것만은 아니다.
예를 들어, 정책을 찾을 때, 엔트로피 값이 큰 정책은 어느 한 행동을 확실하게 선택하지 않고, 확률을 골고루 분배해서 exploration을 꾀할 수 있기 때문이다.</p>
<br>
</section>
</section>
<hr class="docutils" />
<section id="soft-policy-iteration">
<h2><span class="section-number">16.2. </span>Soft policy iteration<a class="headerlink" href="#soft-policy-iteration" title="Permalink to this headline">#</a></h2>
<p>SAC 논문에서는 실용적인 알고리즘인 SAC를 소개하기 전에 policy iteration의 soft 버전인 soft policy iteration을 소개한다.
Policy iteration은 주어진 정책의 행동가치함수를 계산하는 <strong>policy evaluation</strong>과 계산한 행동가치함수를 바탕으로 정책을 개선하는 <strong>policy improvement</strong>를 반복하며 optimal policy를 찾는 방법이다.</p>
<br>
<section id="soft-policy-evaluation">
<h3><span class="section-number">16.2.1. </span>Soft policy evaluation<a class="headerlink" href="#soft-policy-evaluation" title="Permalink to this headline">#</a></h3>
<p>일반적인 policy evaluation과 다르게, soft policy iteration에서는 주어진 정책 <span class="math notranslate nohighlight">\(\pi\)</span>의 soft state value를 다음과 같이 정의한다.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{matrix}
V^\pi(s) &amp; = &amp; \mathbb{E}_{a \sim \pi(\cdot | s)}\left[ Q^\pi(s, a) - \log\pi(a|s)\right] &amp; \qquad (1.1) \\
&amp; = &amp; \mathbb{E}_{a \sim \pi(\cdot | s)}\left[ Q^\pi(s, a) \right] + \mathcal{H}((\pi(\cdot | s))) &amp; \qquad (1.2)
\end{matrix}
\end{split}\]</div>
<br>
<p>식 <span class="math notranslate nohighlight">\((1.1)\)</span>이 상태가치함수의 정의이고, 기댓값의 선형성과 엔트로피의 정의를 사용해서 나타낸 것이 식 <span class="math notranslate nohighlight">\((1.2)\)</span>이다.
원래 우리가 알고 있는 상태가치함수의 성질에서 확률 분포 <span class="math notranslate nohighlight">\(\pi(\cdot | s)\)</span>의 엔트로피만 추가된 형태이다.
즉, 상태 <span class="math notranslate nohighlight">\(s\)</span>에서의 정책 <span class="math notranslate nohighlight">\(\pi\)</span>의 가치는 행동가치함수의 기댓값에 엔트로피를 보너스로 얹어준 것이다.
엔트로피가 클수록 상태 <span class="math notranslate nohighlight">\(s\)</span>에서의 가치함수도 커진다는 것을 알 수 있다.
다양한 행동을 많이 할수록 더욱 가치가 높은 상태이다. 가치함수의 정의를 바꿔줬다는 점이 정말 획기적인 것 같다.</p>
<br>
<p>Soft policy evaluation은 임의로 초기화된 함수 <span class="math notranslate nohighlight">\(Q:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\)</span>에서 시작하여,
다음 벨만 오퍼레이터 <span class="math notranslate nohighlight">\(\mathcal{T}^{\pi}\)</span>를 반복적으로 적용하여 soft Q-value를 계산한다.</p>
<div class="math notranslate nohighlight">
\[
\mathcal{T}^{\pi}Q(s,a) = r(s,a)+\gamma\mathbb{E}_{s'\sim p(\cdot|s,a)}\left[V(s')\right]. \qquad \qquad (2)
\]</div>
<br>
<p>이렇게 계속 벨만 오퍼레이터를 적용하면 soft Q-value로 수렴한다는 이론과 증명이 논문에 잘 나와있다.
증명은 policy iteration이 optimal policy로 수렴한다는 이론을 잘 알고 있다면 쉽게 이해할 수 있다.
주어진 정책 <span class="math notranslate nohighlight">\(\pi\)</span>에 대해서 엔트로피를 딱 계산할 수 있기 때문에 soft state value의 엔트로피텀은 단순 translation에 지나지 않다는 것을 이용한다.</p>
<br>
</section>
<hr class="docutils" />
<section id="soft-policy-improvement">
<h3><span class="section-number">16.2.2. </span>Soft policy improvement<a class="headerlink" href="#soft-policy-improvement" title="Permalink to this headline">#</a></h3>
<p>우리가 흔히 알고 있는 policy iteration의 policy improvement 단계에서는 각 상태에서 행동가치함수가 가장 높은 행동을 하도록 정책을 수정한다. 즉,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\pi_k(a|s)=\begin{cases}1 &amp; \text{if }a=\operatorname*{argmax}\limits_{a \in \mathcal{A}}Q^{\pi_{k-1}}(s,a) \\ 0 &amp; \text{otherwise}\end{cases}. \qquad \qquad (3)
\end{split}\]</div>
<br>
<p>하지만 SAC에서는 행동가치함수가 높을수록 점점 더 높은 확률을 부여하도록 정책을 수정해나간다. 이를 soft policy improvement라고 부른다. 함수값에 따라 확률을 부여할 수 있는 가장 자연스러운 방법은 지수함수를 사용하는 것이다. Soft policy improvement는 다음과 같이 정책을 개선한다.</p>
<div class="math notranslate nohighlight">
\[
\pi_k(\cdot|s)=\operatorname*{argmin}\limits_{\pi}\operatorname{D}_{\text{KL}}\left(\pi(\cdot|s)\bigg\Vert\frac{\exp\left(Q^{\pi_{k-1}}(s, \cdot)\right)}{Z^{\pi_{k-1}}(s)}\right), \qquad \qquad (4)
\]</div>
<br>
<p>여기서 <span class="math notranslate nohighlight">\(\operatorname{D}_{\text{KL}}\)</span> 은 Kullback-Leibler Divergence, <span class="math notranslate nohighlight">\(Z^{\pi}\)</span> 는 확률분포를 만들어주기 위한 정규화텀이다. 이렇게 찾은 <span class="math notranslate nohighlight">\(\pi_k\)</span> 의 가치함수는 <span class="math notranslate nohighlight">\(\pi_{k-1}\)</span> 의 가치함수보다 항상 크거나 같다는 것이 논문에 잘 증명되어 있다 (monotonic improvement).</p>
<br>
</section>
</section>
<hr class="docutils" />
<section id="soft-actor-critic">
<h2><span class="section-number">16.3. </span>Soft Actor-Critic<a class="headerlink" href="#soft-actor-critic" title="Permalink to this headline">#</a></h2>
<p>이제 SAC에 대해서 소개하고자 한다. Soft policy iteration에 off policy 기반 actor critic를 적용한 것이 SAC라고 생각해도 좋다. 논문에 나와 있는 알고리즘은 다음과 같다.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-11-6-sac/Untitled1.png"><img alt="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-11-6-sac/Untitled1.png" src="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-11-6-sac/Untitled1.png" style="width: 400px;" /></a>
</figure>
<br>
<section id="id3">
<h3><span class="section-number">16.3.1. </span>준비물<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h3>
<p>논문에서 소개하고 있는 SAC에서는 총 5개의 네트워크가 필요하다.</p>
<ul class="simple">
<li><p>1개의 정책 네트워크 <span class="math notranslate nohighlight">\(\pi_{\phi}\)</span></p></li>
<li><p>2개의 상태 가치 네트워크 <span class="math notranslate nohighlight">\(V_\psi\)</span>, <span class="math notranslate nohighlight">\(V_{\bar{\psi}}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\bar{\psi}\)</span> 는 target network로 <span class="math notranslate nohighlight">\(\psi\)</span>를 사용해서 업데이트된다. (알고리즘의 밑에서 세 번째 줄)</p></li>
</ul>
</li>
<li><p>2개의 Q-네트워크 <span class="math notranslate nohighlight">\(Q_{\theta_1}\)</span>, <span class="math notranslate nohighlight">\(Q_{\theta_2}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\theta_1\)</span>과 <span class="math notranslate nohighlight">\(\theta_{2}\)</span> 는 target 관계가 아니며, TD3처럼 overestimation을 줄이기 위해 2개를 유지하는 것이다.</p></li>
<li><p><span class="math notranslate nohighlight">\(\pi_\phi\)</span> 와 <span class="math notranslate nohighlight">\(V_{\psi}\)</span> 를 업데이트 할 때, <span class="math notranslate nohighlight">\(\theta_1\)</span>과 <span class="math notranslate nohighlight">\(\theta_{2}\)</span> 중 더 작은 gradient를 만드는 <span class="math notranslate nohighlight">\(\theta\)</span>를 사용한다.</p></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>실제 SAC 구현에서는 상태 가치 네트워크는 사용되지 않는다.
상태 가치 네트워크는 Q-네트워크를 학습시킬 때 <span class="math notranslate nohighlight">\(Q_\theta(s_t, a_t)\)</span>에 대한 타겟값으로 <span class="math notranslate nohighlight">\(r_t+\gamma V_{\bar{\psi}}(s_{t+1})\)</span>을 만들 때 사용된다.
이 타겟값은 사실 Q-네트워크로도 만들 수 있다.
즉, <span class="math notranslate nohighlight">\(r_t+\gamma Q_{\bar{\theta}}(s_{t+1}, a')\)</span>을 타겟값으로 사용하면 된다.
이때, <span class="math notranslate nohighlight">\(a'\sim \pi_{\phi}({\cdot|s_{t+1}})\)</span>은 현재 정책으로부터 샘플링된다.</p>
</div>
<br>
</section>
<hr class="docutils" />
<section id="soft-state-value-function">
<h3><span class="section-number">16.3.2. </span>Soft state value function 학습<a class="headerlink" href="#soft-state-value-function" title="Permalink to this headline">#</a></h3>
<p>다시 한 번 말하지만 실제 SAC 구현체는 상태 가치 네트워크를 포함하고 있지 않다. 따라서 아래의 내용은 SAC 논문에서 나온 내용을 설명한 것뿐이다.
상태 가치 네트워크는 식 <span class="math notranslate nohighlight">\((1.1)\)</span>을 이용하여 학습된다.
실제 soft state value에 대해서는 식 <span class="math notranslate nohighlight">\((1.1)\)</span>이 성립하기 때문에 좌변과 우변의 차이의 제곱을 최소화하는 것을 목표로 한다.
즉, 상태 가치 네트워크 학습을 위한 목적함수는 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
J_V(\psi)=\mathbb{E}_{\mathbf{s}_t \in \mathcal{D}}\left[ \frac{1}{2}\left(V_\psi(\mathbf{s}_t) - \mathbb{E}_{\mathbf{a}_t \ \sim {\pi_\phi}}\left[ Q_{\theta}(\mathbf{s}_t, \mathbf{a}_t) - \log \pi_{\phi}(\mathbf{a}_t|\mathbf{s}_t) \right] \right)^2 \right]. \qquad \qquad (5)
\]</div>
<br>
<p>기댓값으로 표현되는 식 (5)의 실제 그레디언트를 구하기 어려울 것이기 때문에 샘플 하나로 그레디언트를 추정해보면 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
\hat{\nabla}_\psi J_V(\psi)=\nabla_\psi V_\psi(\mathbf{s}_t)\left( V_\psi(\mathbf{s}_t)-Q_{\theta}(\mathbf{s}_t, \mathbf{a}_t) + \log \pi_{\phi}(\mathbf{a}_t|\mathbf{s}_t) \right). \qquad \qquad (6)
\]</div>
<br>
<p>기댓값 대신 실제 샘플 <span class="math notranslate nohighlight">\((\mathbf{s}_t, \mathbf{a}_t)\)</span>을 대입하고, chain rule을 사용해서 그레디언트를 계산한 것이다.</p>
<br>
</section>
<hr class="docutils" />
<section id="soft-q-function">
<h3><span class="section-number">16.3.3. </span>Soft Q-function 학습<a class="headerlink" href="#soft-q-function" title="Permalink to this headline">#</a></h3>
<p>Soft Q-function은 temporal difference learning을 사용하여 업데이트한다. 즉, soft Q-function 학습을 위한 목적함수는 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
J_Q(\theta)=\mathbb{E}_{(\mathbf{s}_t, \mathbf{a}_t)\sim \mathcal{D}} \left[\frac{1}{2} \left( Q_\theta (\mathbf{s}_t, \mathbf{a}_t) - \hat{Q}_{\theta}(\mathbf{s}_t, \mathbf{a}_t) \right)^2 \right], \qquad \qquad (7)
\]</div>
<br>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\hat{Q}_{\theta}(\mathbf{s}_t, \mathbf{a}_t) = r(\mathbf{s}_t, \mathbf{a}_t)+\gamma \mathbb{E}_{\mathbf{s}_{t+1}\sim p(\cdot|\mathbf{s}_t, \mathbf{a}_t)} \left[ V_{\bar{\psi}}(\mathbf{s}_{t+1}) \right]. \qquad \qquad (8)
\]</div>
<br>
<p>샘플을 이용한 stochastic gradient는 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
\hat{\nabla}_{\theta}J_Q(\theta)=\nabla_{\theta}Q_{\theta}(\mathbf{s}_t, \mathbf{a}_t)\left( Q_{\theta}(\mathbf{s}_t, \mathbf{a}_t)-r(\mathbf{s}_t, \mathbf{a}_t) - \gamma V_{\bar{\psi}}(\mathbf{s}_{t+1})\right). \qquad \qquad (9)
\]</div>
<br>
<p>이때, 타겟값 <span class="math notranslate nohighlight">\(r(\mathbf{s}_t, \mathbf{a}_t) - \gamma V_{\bar{\psi}}(\mathbf{s}_{t+1})\)</span>계산에 사용되는 <span class="math notranslate nohighlight">\(V_{\bar{\psi}}(\mathbf{s}_{t+1})\)</span>은 숫자 취급을 해주기 때문에 파라미터에 대한 그레디언트 계산이 이뤄지지 않는다.
실제 구현에서는 타겟 상태 가치 네트워크 <span class="math notranslate nohighlight">\(V_{\bar{\psi}}(\mathbf{s}_{t+1})\)</span> 대신 타겟 Q-네트워크를 사용하게 된다.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>가치 네트워크를 학습시키기 위하여 타겟값을 계산할 때 사용되는 네트워크를 타겟 네트워크라고 부른다.
타겟 네트워크는 가치 네트워크의 구조랑 완전히 동일하며, 학습에 들어가기 전 가치 네트워크의 파라미터와 완전히 동일하게 파라미터가 초기화된다.
가치 네트워크 학습 동안에 타겟 네트워크의 파라미터는 경사하강법을 통해서는 업데이트되지 않는다.
파라미터가 고정된 채로 사용되다가 특정 주기가 되면 가치 네트워크의 파라미터를 복제해와서 업데이트되거나,
매 스탭마다 타겟 네트워크의 파라미터와 가치네트워크의 파라미터를 가중합하는 방식으로 업데이트된다.
이는 매 업데이트마다 타겟값이 급격하게 바뀌는 것을 방지하고 일관된 타겟값을 제공하기 위함이다.
SAC는 후자의 방법은 선택한다.</p>
</div>
<br>
</section>
<hr class="docutils" />
<section id="policy">
<h3><span class="section-number">16.3.4. </span>Policy 학습<a class="headerlink" href="#policy" title="Permalink to this headline">#</a></h3>
<p>이 부분도 굉장히 흥미롭다. 우리가 알고 있는 policy gradient는 평균 상태가치함수 <span class="math notranslate nohighlight">\(\mathbb{E}_{\pi}\left[V_\pi(s)\right]\)</span>를 최적화하는 것으로부터 유도되었다.
하지만 이 논문에서는 상태 가치 함수의 정의가 다르기 때문에 다른 방식으로 policy를 학습하게 된다.  Soft policy improvement를 목적함수로 사용하여 policy를 학습한다.</p>
<div class="math notranslate nohighlight">
\[
J_{\pi}(\phi)=\mathbb{E}_{\mathbf{s}_t \sim \mathcal{D}}\left[ \operatorname{D}_{\text{KL}}\left( \pi_{\phi}(\cdot|\mathbf{s}_t) \bigg\Vert \frac{\exp (Q_{\theta}(\mathbf{s}_t, \cdot))}{Z_{\theta}(\mathbf{s}_t)}\right) \right]. \qquad \qquad (10)
\]</div>
<br>
<p>KL divergence의 정의는 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{D}_{\text{KL}}(p \Vert q)=\mathbb{E}_{X\sim p(x)}\left[\log \frac{p(X)}{q(X)}\right]. \qquad \qquad (11)
\]</div>
<br>
<p>식 <span class="math notranslate nohighlight">\((11)\)</span>의 정의를 사용하면 식 <span class="math notranslate nohighlight">\((10)\)</span>은 다음과 같이 정리될 수 있다.</p>
<div class="math notranslate nohighlight">
\[
J_{\pi}(\phi)\approx\mathbb{E}_{\mathbf{s}_t \sim \mathcal{D}}\left[ \mathbb{E}_{\mathbf{a}_t\sim\pi_{\phi}(\cdot|\mathbf{s}_t)} \left[\log \pi_{\phi}(\mathbf{a}_t | \mathbf{s}_t) - Q_{\theta}(\mathbf{s}_t, \mathbf{a}_t) \right]\right], \qquad \qquad (12)
\]</div>
<br>
<p>여기서 <span class="math notranslate nohighlight">\(Z_{\theta}\)</span>는 <span class="math notranslate nohighlight">\(\phi\)</span>와 무관하기 때문에 생략했다. 현재 안쪽 기댓값은 <span class="math notranslate nohighlight">\(\mathbf{a}_t \sim \pi_{\phi}(\cdot|\mathbf{s}_t)\)</span>에 대해 계산된다.</p>
<br>
<p>우리는 식 <span class="math notranslate nohighlight">\((12)\)</span> 기댓값의 실제 그레디언트를 직접 구하는대신 경험 데이터 <span class="math notranslate nohighlight">\((\mathbf{s}_t, \mathbf{a}_t)\)</span>를 대입하여 기댓값을 추정하고, 그것의 그레디언트를 계산하고 싶다 (stochatic gradient).
데이터 <span class="math notranslate nohighlight">\((\mathbf{s}_t, \mathbf{a}_t)\)</span>를 대입한 값을 <strong>기댓값의 추정치</strong> 로 사용하는 것은 자연스러운 행위이다.
하지만 <strong>기댓값의 추정치의 그레디언트</strong> 를 계산하는 것은 <span class="math notranslate nohighlight">\(\mathbf{a} \sim \pi_{\phi}(\cdot|\mathbf{s}_t)\)</span> 부분을 커버하지 못한다.
<span class="math notranslate nohighlight">\(\phi\)</span>가 변하면 <span class="math notranslate nohighlight">\(\pi_{\phi}\)</span>가 변하고, 그로 인해 기댓값 계산의 범위 <span class="math notranslate nohighlight">\(\mathbf{a} \sim \pi_{\phi}(\cdot|\mathbf{s}_t)\)</span> 즉각적으로 바뀌기 때문이다.
조금 더 쉽게 설명하자면 <span class="math notranslate nohighlight">\(\pi_{\phi}(\cdot|\mathbf{s}_t)\)</span>의 순간변화를 고려하여 <span class="math notranslate nohighlight">\(\log \pi_{\phi}(\cdot|\mathbf{s}_t)\)</span>의 순간변화율을 구해야 하는데,
stochastic gradient를 사용하면 <span class="math notranslate nohighlight">\(\log \pi_{\phi}(\cdot|\mathbf{s}_t)\)</span>의 순간변화만 고려하게 되는 셈이다.
물론, 그 부분을 감수하고도 stochastic gradient를 그레디언트의 추정량으로 사용할 수 있다.</p>
<br>
<p>논문에서는 reparameterization trick을 사용해서 분산이 낮은 그레디언트를 추정량이 사용하게 된다. 즉, 행동 <span class="math notranslate nohighlight">\(\mathbf{a}_t\)</span>를 다른 랜덤변수 <span class="math notranslate nohighlight">\(\epsilon_t\)</span>를 사용하여 나타내게 만들었다.</p>
<div class="math notranslate nohighlight">
\[
\mathbf{a}_t=f_{\phi}(\epsilon_t; \mathbf{s}_t), \; \text{where} \;\epsilon_t \sim \mathcal{N}(\mathbf{0},  I). \qquad \qquad (13)
\]</div>
<br>
<p>정책에서 <span class="math notranslate nohighlight">\(\pi_{\phi}(\cdot|\mathbf{s}_t)\)</span>에서 행동 <span class="math notranslate nohighlight">\(\mathbf{a}_t\)</span>을 샘플링하는 대신 가우시안 분포에서 <span class="math notranslate nohighlight">\(\epsilon_t\)</span>를 샘플링하고 이를 <span class="math notranslate nohighlight">\(f_{\phi}\)</span>에 대입하여 행동을 계산하겠다는 의미이다. 식 <span class="math notranslate nohighlight">\((13)\)</span>을 식 <span class="math notranslate nohighlight">\((12)\)</span>에 대입하면 다음과 같이 정리된다.</p>
<div class="math notranslate nohighlight">
\[
J_{\pi}(\phi)\approx\mathbb{E}_{\mathbf{s}_t \sim \mathcal{D}}\left[ \mathbb{E}_{\epsilon_t \sim \mathcal{N}} \left[\log \pi_{\phi}(f_{\phi}(\epsilon_t; \mathbf{s}_t) | \mathbf{s}_t) - Q_{\theta}(\mathbf{s}_t, f_{\phi}(\epsilon_t; \mathbf{s}_t)) \right]\right], \qquad \qquad (14)
\]</div>
<br>
<p>이제 식 <span class="math notranslate nohighlight">\((14)\)</span>의 안쪽 기댓값은 <span class="math notranslate nohighlight">\(\epsilon_t \sim \mathcal{N}(\mathbb{0}, I)\)</span>에 대해 계산되기 때문에 <span class="math notranslate nohighlight">\(\phi\)</span> 변화에 따른 <span class="math notranslate nohighlight">\(\mathbf{a} \sim \pi_{\phi}(\cdot|\mathbf{s}_t)\)</span> 변화를 고려하지 않아도 된다. 식 <span class="math notranslate nohighlight">\((14)\)</span>에 경험 데이터 <span class="math notranslate nohighlight">\(\mathbf{s}_t\)</span>와 샘플링한 <span class="math notranslate nohighlight">\(\epsilon_t\)</span>를 대입하여 <span class="math notranslate nohighlight">\(\phi\)</span>에 대한 그레디언트를 계산하면 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
\hat{\nabla}_{\phi}J_{\pi_{\phi}}(\phi)=\nabla_{\phi} \log \pi_{\phi}(\mathbf{a}_t | \mathbf{s}_t) + \nabla_{\phi}f_{\phi}(\epsilon_t; \mathbf{s}_t)(\nabla_{\mathbf{a}_{t}}\log \pi_{\phi}(\mathbf{a}_t | \mathbf{s}_t) - \nabla_{\mathbf{a}_t}Q_{\theta}(\mathbf{s}_t, \mathbf{a}_t)), \qquad \qquad (15)
\]</div>
<br>
<p>여기서 <span class="math notranslate nohighlight">\(\mathbf{a}_t\)</span>는 샘플링한 <span class="math notranslate nohighlight">\(\epsilon_t\)</span>을 사용해서  <span class="math notranslate nohighlight">\(\mathbf{a}_t = f_{\phi}(\epsilon_t;\mathbf{s}_t)\)</span>으로 계산한 것이다. 우리가 알고 있는 체인룰에 의하면 식 <span class="math notranslate nohighlight">\((15)\)</span> 우변의 두 번째 텀만 있어야 할 것 같다. 첫 번째 텀 <span class="math notranslate nohighlight">\(\nabla_{\phi} \log \pi_{\phi}(\mathbf{a}_t | \mathbf{s}_t)\)</span>는 어디서 등장한 것일까? 바로 total derivative 개념 때문이다.</p>
<br>
<p>그럼 total derivative에 대해 간략히 알아보자.  설명을 위해 등장하는 <span class="math notranslate nohighlight">\(f\)</span>는 잠시 동안만 사용될 지역변수이다. Reparameterization에서 설명한 <span class="math notranslate nohighlight">\(f_{\phi}\)</span>가 아니다.</p>
<br>
<p>어떤 함수 <span class="math notranslate nohighlight">\(f\)</span>가 변수 <span class="math notranslate nohighlight">\(x\)</span>와 <span class="math notranslate nohighlight">\(y\)</span>에 의존적일 때 우리는 <span class="math notranslate nohighlight">\(f(x, y)\)</span>으로 적어준다. 쉬운 이해를 위해 <span class="math notranslate nohighlight">\(f(x, y) = xy + y^2\)</span>인 함수 <span class="math notranslate nohighlight">\(f\)</span>가 있다고 하자. 함수 <span class="math notranslate nohighlight">\(f\)</span>를 <span class="math notranslate nohighlight">\(x\)</span>에 대해 편미분할 때 우리는 <span class="math notranslate nohighlight">\(y\)</span>를 상수 취급한다. 즉,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial x} = y.
\]</div>
<br>
<p>그런데 만약 <span class="math notranslate nohighlight">\(y\)</span>가 <span class="math notranslate nohighlight">\(x\)</span>에 의존적이면 어떨까? 예를 들어, <span class="math notranslate nohighlight">\(y = x^2\)</span>인 의존성이 있다면 어떨까? <span class="math notranslate nohighlight">\(x\)</span>가 변하면 <span class="math notranslate nohighlight">\(y\)</span>도 변하고, <span class="math notranslate nohighlight">\(x\)</span>와 <span class="math notranslate nohighlight">\(y\)</span>의 변화에 의해 <span class="math notranslate nohighlight">\(f\)</span>가 변한다. 변수와 변수 사이의 의존성까지 고려하여 계산하는 미분을 total derivative라고 한다. 함수 <span class="math notranslate nohighlight">\(f\)</span>에 대한 <span class="math notranslate nohighlight">\(x\)</span>의 total derivative는 다음과 같이 계산된다.</p>
<div class="math notranslate nohighlight">
\[
\frac{df}{dx}=\frac{\partial f}{\partial x} + \frac{\partial f}{\partial y}\frac{dy}{dx}=y + x + 4 x = y + 5x. \qquad \qquad (*)
\]</div>
<br>
<p><span class="math notranslate nohighlight">\(x\)</span>에 의한 <span class="math notranslate nohighlight">\(f\)</span>의 변화와 <span class="math notranslate nohighlight">\(x\)</span>에 대한 <span class="math notranslate nohighlight">\(y\)</span>의 변화 그리고 그것으로 인한 <span class="math notranslate nohighlight">\(f\)</span>을 변화를 모두 고려하게 되어 준다.</p>
<p>다시 돌아와서 식 <span class="math notranslate nohighlight">\((14)\)</span>을 <span class="math notranslate nohighlight">\(\phi\)</span>에 대해 미분해 줄 때, 체인룰의 중간 변수 <span class="math notranslate nohighlight">\(\mathbf{a}_t=f_{\phi}(\epsilon_t;\mathbf{s}_t)\)</span>가 <span class="math notranslate nohighlight">\(\phi\)</span>에 의존적인 상황이다. 따라서 편미분인 아닌 total derivative를 계산해줘야 한다. 식 <span class="math notranslate nohighlight">\((15)\)</span> 우변의 첫 번째 텀 <span class="math notranslate nohighlight">\(\nabla_{\phi} \log \pi_{\phi}(\mathbf{a}_t | \mathbf{s}_t)\)</span>이 식 <span class="math notranslate nohighlight">\((*)\)</span>의 <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x}\)</span>에 해당하는 부분이다. 그리고 식 <span class="math notranslate nohighlight">\((15)\)</span> 우변의 두 번째 텀은 식 <span class="math notranslate nohighlight">\((*)\)</span>의 <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y}\frac{dy}{dx}\)</span>에 해당하는 부분이며, 우리가 아는 체인룰 느낌이 난다.</p>
<br>
<p>지금까지 SAC의 각 네트워크의 손실함수와 그레디언트에 알아보았다. 지금까지의 내용을 사용해서 SAC 알고리즘을 수행하면 된다.</p>
<br>
</section>
</section>
<hr class="docutils" />
<section id="experiment">
<h2><span class="section-number">16.4. </span>Experiment<a class="headerlink" href="#experiment" title="Permalink to this headline">#</a></h2>
<p>SAC 논문에서는 OpenAI gym benchmark suite와 rllab의 Humanoid 환경을 사용하여 실험을 하였다. 논문에서는 다음 다섯 가지 알고리즘을 비교하고 있다.</p>
<ul class="simple">
<li><p>DDPG: off-policy, actor-critic</p></li>
<li><p>PPO: on-policy, policy</p></li>
<li><p>Soft Q-learning (SQL):  off-policy, Q-learning for learning maximum entropy policies with two Q-networks</p></li>
<li><p>TD3: off-policy, actor-critic, two Q-networks</p></li>
<li><p>SAC: off-policy, actor-critic, two Q-networks for learning maximum entropy policies</p></li>
</ul>
<br>
<p>결과는 다음과 같다.</p>
<figure class="align-default">
<img alt="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-11-6-sac/Untitled2.png" src="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-11-6-sac/Untitled2.png" />
</figure>
<br>
<p>특히, Humanoid 환경의 경우 21차원의 행동공간을 갖는 복잡한 환경이다. 많은 off-policy 알고리즘들은 학습 불안정성 때문에 humanoid 환경을 해결하지 못한다. 하지만 SAC는 soft policy improvement를 통해 stochastic policy를 학습하게 되는데, 이로 인해 안정적인 학습이 가능하여 humanoid 환경을 해결할 수 있었다. Stochatic policy가 학습 안정성을 더한다는 것을 보이기 위하여 SAC의 deterministic policy 변형체를 만들어서 humanoid에 실험을 했는데, 결과는 다음과 같다.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-11-6-sac/Untitled3.png"><img alt="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-11-6-sac/Untitled3.png" src="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-11-6-sac/Untitled3.png" style="width: 400px;" /></a>
</figure>
<br>
<p>SAC에서 중요한 하이퍼 파라미터는 reward scale이다. 뜬금 없이 보상의 스케일이 중요한 이유는 보상의 스케일에 따라, 식 <span class="math notranslate nohighlight">\((1)\)</span>에 있는 엔트로피텀의 영향력이 달라지기 때문이다. 보상의 스케일을 달리하며 Ant-v1에 실험한 결과는 다음 그림의 가운데에 나타나 있다. 참고로 <span class="math notranslate nohighlight">\((a)\)</span>는 학습이 종료된 후 evaluation rollout에서 stochastice policy를 사용할지 또는 exploitation을 위해 deterministic policy를 사용할지에 따른 결과이다. <span class="math notranslate nohighlight">\((c)\)</span>는 타겟 가치함수 업데이트에 사용되는 <span class="math notranslate nohighlight">\(\tau\)</span> 파라미터에 대한 그림이다.</p>
<figure class="align-default">
<img alt="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-11-6-sac/Untitled4.png" src="https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2022-11-6-sac/Untitled4.png" />
</figure>
<br>
<hr class="docutils" />
<p>다음 장에서는 SAC를 직접 구현해보도록 할 것이다.</p>
<br>
<script
   type="text/javascript"
   src="https://utteranc.es/client.js"
   async="async"
   repo="HiddenBeginner/Deep-Reinforcement-Learnings"
   issue-term="pathname"
   theme="github-light"
   label="💬 comment"
   crossorigin="anonymous"
/></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./book/Chapter2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="10-implementation-trpo.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">15. </span>TRPO 구현</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../Reference.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">참고문헌</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 재야의 숨은 초보<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>