
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>6. Policy Gradient Theorem &#8212; 심층강화학습</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "HiddenBeginner/Deep-Reinforcement-Learnings");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/HDBG.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="7. REINFORCE" href="2-reinforce.html" />
    <link rel="prev" title="5. 가치 함수 근사하기: Stochastic approximation" href="../Chapter1/5-stochastic-approximation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-FTQEC31PV8"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-FTQEC31PV8');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/HDBG.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">심층강화학습</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    심층강화학습 (Deep Reinforcement Learnings)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/1-sequential-decision-making-problems.html">
   1. 순차적 의사 결정 문제, 에이전트, 환경
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/2-markov-decision-processes.html">
   2. Markov Decision Process (MDP)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/3-policy-return-value.html">
   3. 정책, Return, 가치 함수
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/4-bellman-equation.html">
   4. 벨만 방정식: 가치 함수의 재귀적 성질
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1/5-stochastic-approximation.html">
   5. 가치 함수 근사하기: Stochastic approximation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Policy gradient methods
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   6. Policy Gradient Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-reinforce.html">
   7. REINFORCE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-implementation-reinforce.html">
   8. REINFORCE 구현
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-reinforce-with-baseline.html">
   9. REINFORCE with baseline
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  참고문헌
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Reference.html">
   참고문헌
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/hiddenbeginner/Deep-Reinforcement-Learnings"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/hiddenbeginner/Deep-Reinforcement-Learnings/issues/new?title=Issue%20on%20page%20%2Fbook/Chapter2/1-policy-gradient-theorem.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/book/Chapter2/1-policy-gradient-theorem.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   6.1. Policy Gradient Theorem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   6.2. Policy Gradient Theorem 증명
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Policy Gradient Theorem</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   6.1. Policy Gradient Theorem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   6.2. Policy Gradient Theorem 증명
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="policy-gradient-theorem">
<h1><span class="section-number">6. </span>Policy Gradient Theorem<a class="headerlink" href="#policy-gradient-theorem" title="Permalink to this headline">#</a></h1>
<p>강화학습에서 정책 (policy)은 주어진 상태에서 어떤 행동을 취할지를 알려주는 일종의 지침서 같은 것이다. 보다 더 일반적으로는, 정책 <span class="math notranslate nohighlight">\(\pi\)</span>는 주어진 상태 <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>에서 어떤 행동 <span class="math notranslate nohighlight">\(a \in \mathcal{A}\)</span>을 선택할 조건부 확률이다. 즉, <span class="math notranslate nohighlight">\(\pi(a | s) = \text{Pr} \left[ A_t = a | S_t = s \right]\)</span> 이다. 만약 상태의 개수와 행동의 개수가 적다면 사람이 직접 각 <span class="math notranslate nohighlight">\((s, a)\)</span>마다 확률을 부여하여 정책을 만들 수 있을 것이다. 하지만, 대부분의 환경은 가능한 상태와 행동의 개수가 굉장히 많으며, 심지어 부여할 수 있는 확률 값도 정말 무수히 많을 것이다. 이런 고생을 덜고자 매개변수화된 함수로 정책을 모델링하여 좋은 정책을 찾는 방법을 <strong>policy-based</strong> 방법이라고 한다. 매개변수를 <span class="math notranslate nohighlight">\(\theta \in \mathbb{R}^{d}\)</span>이라고 하면, 이제 매개변수화된 정책은 다음과 같이 적어줄 수 있다.</p>
<div class="math notranslate nohighlight">
\[\pi_\theta(a | s) = \text{Pr} \left[ A_t = a | S_t =s,\theta_t=\theta \right].\]</div>
<br>
<p>매개변수의 값에 따라 정책의 성능이 좋을 수도 있고 나쁠 수도 있을 것이다. 우리의 목표는 좋은 정책을 만드는 매개변수를 찾는 것이다. 그러기 위해선 정책의 성능을 평가하는 성능 지표 (performance measure)가 필요하다. 매개변수에 따라 정책의 성능이 달라지므로 성능 지표는 매개변수 값에 의해 결정된다. 따라서 성능 지표를 매개변수에 대한 함수 <span class="math notranslate nohighlight">\(J(\theta)\)</span>로 적어준다.</p>
<br>
<p>우리는 성능 지표를 크게 만들어주는 매개변수를 찾기 위하여 매개변수에 대한 성능 지표의 그레디언트를 계산하고 경사하강법을 사용할 것이다.</p>
<div class="math notranslate nohighlight">
\[\theta_{\text{new}}=\theta_{\text{old}}+\alpha\widehat{\nabla}_{\theta}{J(\theta_{\text{old}})}\]</div>
<br>
<p>실제 그레디언트 <span class="math notranslate nohighlight">\(\nabla_{\theta} J(\theta_{\text{old}})\)</span>을 찾을 수 있으면 베스트이지만, 일반적으로는 그레디언트에 대한 stochastic 추정치 <span class="math notranslate nohighlight">\(\widehat{\nabla}_{\theta}{J(\theta_{\text{old}})}\)</span>를 사용한다. 이때 stochastic 추정량 <span class="math notranslate nohighlight">\(\widehat{\nabla}_{\theta}{J(\theta_{\text{old}})}\)</span>의 기댓값이 실제 그레디언트 <span class="math notranslate nohighlight">\(\nabla_{\theta}{J(\theta_{\text{old}})}\)</span>에 근사하는 추정량을 사용해야 할 것이다. 이와 같이 그레디언트를 사용하여 좋은 정책을 학습하는 방법을 <strong>policy gradient</strong> 방법이라고 부른다.</p>
<br>
<hr class="docutils" />
<section id="id1">
<h2><span class="section-number">6.1. </span>Policy Gradient Theorem<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<p>우리는 정책의 성능을 평가하는 지표 <span class="math notranslate nohighlight">\(J(\theta)\)</span>의 그레디언트를 사용하여 점점 더 좋은 정책을 찾아나갈 것이다. 그럼, 가장 먼저 성능 지표 <span class="math notranslate nohighlight">\(J(\theta)\)</span>를 정의해야 한다. 이 성능 지표는 주어진 MDP의 설정에 따라 달라질 수 있다. 성능 지표가 달라지면, 그레디언트도 달라질 것이다. 그럼 우리는 성능 지표를 정의할 때마다 그레디언트를 해석적으로 (analytically, 직접 식을 전개하여 푸는 것을 의미) 계산을 해야 하는가? 정말 다행히도 policy gradient theorem은 다양한 성능 지표에 대해서 그레디언트들이 서로 비례한다는 것을 보였다.</p>
<br>
<p>Policy gradient theorem을 조금 더 쉽게 기술하기 위해 주어진 MDP가 유한 상태 공간, 유한 행동 공간 갖는다고 가정할 것이다. 생각해볼 수 있는 가장 자연스러운 정책 평가 지표는 에피소드 동안 받은 보상의 총합의 기댓값일 것이다. 즉, 초기 상태의 가치 함수이다. 초기 상태 확률 분포에 따라 초기 상태가 다양하게 있을 수 있으므로 기댓값을 취하는 것이 좋을 것이다.</p>
<div class="math notranslate nohighlight" id="equation-objective">
<span class="eqno">(6.1)<a class="headerlink" href="#equation-objective" title="Permalink to this equation">#</a></span>\[J(\theta):= \mathbb{E}_{S_0 \sim d_0} \left[ V^{\pi_{\theta}}(S_0) \right].\]</div>
<br>
<p>자, 이제 식 <a class="reference internal" href="#equation-objective">(6.1)</a>의 그레디언트를 계산해보자. 사실, 썩 쉬워보이지 않는다. 우선, <span class="math notranslate nohighlight">\(J(\theta)\)</span>는 정책이 취하는 행동에 따라 달라질 수 있다. 그리고, 정책을 따랐을 때 방문하는 상태들에 따라서도 달라질 수 있다. 그래, 정책은 <span class="math notranslate nohighlight">\(\theta\)</span>에 대한 함수니깐 그레디언트를 구할 수 있을 것이다. 하지만 정책이 방문한 상태들의 분포는 정책 뿐만 아니라 환경의 transition 모델에 따라 달라질 수 있기 때문에 그레디언트를 계산하는 것이 만만치 않을 것이다.</p>
<br>
<p>정말 다행히도 식 <a class="reference internal" href="#equation-objective">(6.1)</a>의 그레디언트를 다음과 같이 쉽게 구할 수 있다는 이론이 <strong>policy gradient theorem</strong>이다.</p>
<div class="math notranslate nohighlight" id="equation-policy-gradient-theorem">
<span class="eqno">(6.2)<a class="headerlink" href="#equation-policy-gradient-theorem" title="Permalink to this equation">#</a></span>\[\nabla_{\theta} J(\theta) \propto \sum_{s \in \mathcal{S}} d_{\pi_{\theta}}(s) \sum_{a \in \mathcal{A}} Q^{\pi_{\theta}}(s,a) \nabla_{\theta} \pi_{\theta}(a|s),\]</div>
<br>
<p>여기서 <span class="math notranslate nohighlight">\(d_{\pi_{\theta}}(s)\)</span>는 정책 <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span>를 따랐을 때 상태 <span class="math notranslate nohighlight">\(s\)</span>에 머무를 확률로 이해하면 된다 (아래 증명에 더 상세히 정의된다). 식 <a class="reference internal" href="#equation-policy-gradient-theorem">(6.2)</a>는 여전히 복잡해 보이지만, 우려와 다르게 방문한 상태들의 분포 <span class="math notranslate nohighlight">\(d_{\pi_{\theta}}(s)\)</span> 를 미분하는 일은 발생하지 않았다.
그리고 <span class="math notranslate nohighlight">\(d_{\pi_{\theta}}(s)\)</span>는 정책 <span class="math notranslate nohighlight">\(\pi_\theta\)</span>를 사용하여 에피소드를 굉장히 많이 진행하여 Monte-Carlo 방식으로 얼추 추정할 수 있을 것이다.
한편, 책에는 나와있지 않지만 식 <a class="reference internal" href="#equation-policy-gradient-theorem">(6.2)</a>을 다음과 같이도 나타낼 수 있다.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{matrix}
\nabla_{\theta} J(\theta) &amp; \propto &amp; \sum_s d_{\pi_{\theta}}(s) \sum_{a} Q^{\pi_{\theta}}(s,a) \nabla_{\theta} \pi_{\theta}(a|s) &amp; \\
&amp; = &amp; \sum_s d_{\pi_{\theta}}(s) \sum_{a} \pi(a|s) Q^{\pi_{\theta}}(s,a) \frac{\nabla_{\theta} \pi_{\theta}(a|s)}{\pi_{\theta}(a|s)}  &amp; \quad (a) \\
&amp; = &amp; \sum_s d_{\pi_{\theta}}(s) \sum_{a} \pi(a|s) Q^{\pi_{\theta}}(s,a) \nabla_{\theta} \log \pi_{\theta}(a|s)  &amp; \quad (b) \\
&amp; = &amp; \mathbb{E}_{\pi_{\theta}} \left[  Q^{\pi_{\theta}}(S_t, A_t) \nabla_{\theta} \log \pi_{\theta}(A_t|S_t) \right]  &amp; \quad (c) \\
\end{matrix}
\end{split}\]</div>
<br>
<p><span class="math notranslate nohighlight">\((a)\)</span>은 그냥 <span class="math notranslate nohighlight">\(\frac{\pi_{\theta}(a|s)}{\pi_{\theta}(a|s)}\)</span>를 곱해주고 위치만 바꾼 것이다. <span class="math notranslate nohighlight">\((b)\)</span>는 <span class="math notranslate nohighlight">\(\frac{d}{dx} \log f(x)=\frac{f'(x)}{f(x)}\)</span>임을 사용한 것이다.
마지막으로 <span class="math notranslate nohighlight">\((c)\)</span>는 <span class="math notranslate nohighlight">\(\mathbb{E}\left[ X \right] = \sum_{x}x\;p(x)\)</span>임을 사용한 것인데, 확률 <span class="math notranslate nohighlight">\(p(x)\)</span>에 해당하는 부분은 <span class="math notranslate nohighlight">\(d_{\pi_{\theta}}(s)\pi_{\theta}(a|s)\)</span>이고, 확률변수 <span class="math notranslate nohighlight">\(X\)</span>에 해당하는 부분이 <span class="math notranslate nohighlight">\(Q^{\pi_{\theta}}(S_t,A_t) \nabla \log \pi_{\theta}(A_t|S_t)\)</span>이다. 확률변수 (random variable)은 대문자, 결과 (outcome)은 소문자로 표기해주었다. 식 <span class="math notranslate nohighlight">\((c)\)</span>처럼 적어주면 좋은 이유는, 실제 기댓값은 구하기 어렵겠지만, 에피소드를 많이 반복하여  <span class="math notranslate nohighlight">\(Q^{\pi_{\theta}}(s,a) \nabla \log \pi_{\theta}(a|s)\)</span>를 얻고 표본 평균을 내어 실제 기댓값에 근사할 수 있다는 것이다. 그리고 식 <span class="math notranslate nohighlight">\((c)으\)</span>로 보는 것이 이후 REINFORCE나 Actor-Crtic 알고리즘을 설명할 때 더 용이하다.</p>
<br>
</section>
<hr class="docutils" />
<section id="id2">
<h2><span class="section-number">6.2. </span>Policy Gradient Theorem 증명<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<p>증명의 편의성을 위하여 유한 상태 공간 및 유한 행동 공간임을 가정하자. 연속 공간일 경우 summation을 적분으로 바꿔주면 된다. 먼저, 우리의 목적 함수는 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
J(\theta) := \mathbb{E}_{S_0 \sim d_0}\left[ V^{\pi_\theta}(S_0) \right]=\sum_{s_0 \in \mathcal{S}}d_0(s_0)V^{\pi_\theta}(s_0).
\]</div>
<br>
<p>목적함수를 최대화하기 위하여 우리는 gradient ascent를 사용할 것이며, gradient ascent를 위해서는 목적함수의 그레디언트를 계산해야 한다. 양변에 그레디언트를 취해보자.</p>
<div class="math notranslate nohighlight">
\[
\nabla_\theta J(\theta) =  \nabla_{\theta} \sum_{s_0 \in \mathcal{S}}d_0(s_0)V^{\pi_\theta}(s_0) = \sum_{s_0 \in \mathcal{S}}d_0(s_0) \nabla_{\theta}V^{\pi_\theta}(s_0).
\]</div>
<br>
<p>위 식에서 <span class="math notranslate nohighlight">\(d_0\)</span>에는 <span class="math notranslate nohighlight">\(\theta\)</span>가 없어서 상수로 취급하고, <span class="math notranslate nohighlight">\(\theta\)</span>에 종속적인 <span class="math notranslate nohighlight">\(V^{\pi_\theta}(s_0)\)</span>만 그레디언트를 취해준 것이다. 먼저, 상태가치함수는 행동가치함수의 기댓값이라는 성질을 이용하자. 즉, 다음과 같은 등식이 모든 <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>, <span class="math notranslate nohighlight">\(a \in \mathcal{A}\)</span>에 대해 성립한다.</p>
<div class="math notranslate nohighlight">
\[
V^{\pi} (s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q^{\pi}(s, a).
\]</div>
<br>
<p>위 성질을 <span class="math notranslate nohighlight">\(V^{\pi_\theta}(s_0)\)</span>에 대입하자.</p>
<div class="math notranslate nohighlight">
\[
\nabla_\theta J(\theta) =  \ \sum_{s_0 \in \mathcal{S}}d_0(s_0) \nabla_{\theta}\left( \sum_{a_0 \in \mathcal{A}} \pi_\theta(a_0|s_0)Q^{\pi_\theta}(s_0, a_0) \right).
\]</div>
<br>
<p>위 식에서 <span class="math notranslate nohighlight">\(\theta\)</span> 에 종속적인 부분은 <span class="math notranslate nohighlight">\(\pi_\theta(a_0|s_a)Q^{\pi_\theta}(s_0, a_0)\)</span>이다. 미분의 곱셈 법칙을 사용하자.</p>
<div class="math notranslate nohighlight">
\[
\nabla_\theta J(\theta) =  \ \sum_{s_0 \in \mathcal{S}}d_0(s_0) \sum_{a_0 \in \mathcal{A}} \left( \nabla_\theta\pi_\theta(a_0|s_0)Q^{\pi_\theta}(s_0, a_0) + \pi_\theta(a_0|s_0) \nabla_\theta Q^{\pi_\theta}(s_0, a_0) \right).
\]</div>
<br>
<p>이제 행동가치함수의 재귀적 성질을 이용하자. 즉, 다음 성질을 이용할 것이다.</p>
<div class="math notranslate nohighlight">
\[
Q^{\pi}(s, a) = r + \gamma \sum_{s'}p(s' | s, a) V^{\pi}(s'), \text{ where } r=r(s,a).
\]</div>
<br>
<p>위 성질을 <span class="math notranslate nohighlight">\(Q^{\pi_\theta}(s_0, a_0)\)</span>에 대입하자.</p>
<div class="math notranslate nohighlight">
\[
\nabla_\theta J(\theta) =  \ \sum_{s_0 \in \mathcal{S}}d_0(s_0) \sum_{a_0 \in \mathcal{A}} \left( \nabla_\theta\pi_\theta(a_0|s_0)Q^{\pi_\theta}(s_0, a_0) + \pi_\theta(a_0|s_0) \nabla_\theta \left( r_0 + \gamma \sum_{s_1 \in \mathcal{S}}p(s_1 | s_0, a_0) V^{\pi_\theta}(s_1)  \right)  \right).
\]</div>
<br>
<p><span class="math notranslate nohighlight">\(r_0=r(s_0, a_0)\)</span>은 보상함수로부터 계산되기 때문에 <span class="math notranslate nohighlight">\(\theta\)</span>에 종속적이지 않다. 따라서 그레디언트가 취해지면 0이 된다. <span class="math notranslate nohighlight">\(V^{\pi_\theta}(s_1)\)</span>는 <span class="math notranslate nohighlight">\(\theta\)</span>에 종속적이기 때문에 그레디언트를 취해줘야 한다. 즉, 다음과 같이 정리된다.</p>
<div class="math notranslate nohighlight">
\[
\nabla_\theta J(\theta) =   \sum_{s_0 \in \mathcal{S}}d_0(s_0) \sum_{a_0 \in \mathcal{A}} \left( \nabla_\theta \pi_\theta(a_0|s_0)Q^{\pi_\theta}(s_0, a_0) + \gamma  \pi_\theta(a_0|s_0) \sum_{s_1 \in \mathcal{S}} p(s_1 | s_0, a_0) \nabla_\theta V^{\pi_\theta}(s_1)\right).
\]</div>
<br>
<p>잠시 짚고 넘어가자면, 우리는 처음에 <span class="math notranslate nohighlight">\(\nabla_\theta V^{\pi_\theta}(s_0)\)</span>부터 시작해서 가치함수의 성질을 이용한 전개를 통해 위 식까지 도달한 것이다.
즉, <span class="math notranslate nohighlight">\(\nabla_\theta V^{\pi_\theta}(s_0)\)</span>을 전개해보면 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
\nabla_\theta V^{\pi_\theta}(s_0)=\sum_{a_0 \in \mathcal{A}} \left( \nabla_\theta \pi_\theta(a_0|s_0)Q^{\pi_\theta}(s_0, a_0) + \gamma  \pi_\theta(a_0|s_0) \sum_{s_1 \in \mathcal{S}} p(s_1 | s_0, a_0) \nabla_\theta V^{\pi_\theta}(s_1)\right),
\]</div>
<p>으로 전개해주었다.</p>
<br>
<p>한편, 지금까지의 <span class="math notranslate nohighlight">\(\nabla_\theta J(\theta)\)</span> 식에는 크게 두 항이 있다.</p>
<div class="math notranslate nohighlight">
\[
\sum_{s_0 \in \mathcal{S}}d_0(s_0)\sum_{a_0 \in \mathcal{A}} \nabla_\theta \pi_\theta(a_0|s_0)Q^{\pi_\theta}(s_0, a_0),
\]</div>
<div class="math notranslate nohighlight">
\[
\gamma \sum_{s_0 \in \mathcal{S}}d_0(s_0) \sum_{a_0 \in \mathcal{A}}   \pi_\theta(a_0|s_0) \sum_{s_1 \in \mathcal{S}} p(s_1 | s_0, a_0) \nabla_\theta V^{\pi_\theta}(s_1).
\]</div>
<br>
<p>두 번째 항에 <span class="math notranslate nohighlight">\(\nabla_\theta V^{\pi_\theta}(s_1)\)</span>이 있다. <span class="math notranslate nohighlight">\(\nabla_\theta V^{\pi_\theta}(s_0)\)</span>에 대해 했던 것처럼 <span class="math notranslate nohighlight">\(\nabla_\theta V^{\pi_\theta}(s_1)\)</span>을 전개해보면 다음과 같을 것이다.</p>
<div class="math notranslate nohighlight">
\[
\nabla_\theta V^{\pi_\theta}(s_1)=\sum_{a_1 \in \mathcal{A}} \left( \nabla_\theta \pi_\theta(a_1|s_1)Q^{\pi_\theta}(s_1, a_1) + \gamma  \pi_\theta(a_1|s_1) \sum_{s_2 \in \mathcal{S}} p(s_2 | s_1, a_1) \nabla_\theta V^{\pi_\theta}(s_2)\right),
\]</div>
<br>
<p>아래 첨자를 1씩 증가시켜준 것이다. 이를 <span class="math notranslate nohighlight">\(\nabla_\theta V^{\pi_\theta}(s_1)\)</span>에 대입해주자.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_\theta
 J(\theta) &amp; = &amp; \sum_{s_0 \in \mathcal{S}}d_0(s_0)  \sum_{a_0 \in \mathcal{A}} \Bigg( \nabla_\theta \pi_\theta(a_0|s_0)Q^{\pi_\theta}(s_0, a_0) + \gamma  \pi_\theta(a_0|s_0) \sum_{s_1 \in \mathcal{S}} p(s_1 | s_0, a_0)  \bigg( \\&amp;&amp;
 \sum_{a_1 \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a_1|s_1)Q^{\pi_\theta}(s_1, a_1) + \gamma  \pi_\theta(a_1|s_1) \sum_{s_2 \in \mathcal{S}} p(s_2 | s_1, a_1) \nabla_\theta V^{\pi_\theta}(s_2) \Big) \bigg) \Bigg).
\end{align*}
\end{split}\]</div>
<br>
<p>그리고 위 식에는 총 3항이 있다.</p>
<div class="math notranslate nohighlight">
\[
\sum_{s_0 \in \mathcal{S}}d_0(s_0)\sum_{a_0 \in \mathcal{A}} \nabla_\theta \pi_\theta(a_0|s_0)Q^{\pi_\theta}(s_0, a_0),
\]</div>
<div class="math notranslate nohighlight">
\[
\gamma \sum_{s_0 \in \mathcal{S}}d_0(s_0)\sum_{a_0 \in \mathcal{A}} \pi_\theta(a_0|s_0) \sum_{s_1 \in \mathcal{S}} p(s_1 | s_0, a_0) \sum_{a_1 \in \mathcal{A}}\nabla_\theta \pi_\theta(a_1|s_1)Q^{\pi_\theta}(s_1, a_1),
\]</div>
<div class="math notranslate nohighlight">
\[
\gamma^2 \sum_{s_0 \in \mathcal{S}}d_0(s_0)\sum_{a_0 \in \mathcal{A}} \pi_\theta(a_0|s_0) \sum_{s_1 \in \mathcal{S}} p(s_1 | s_0, a_0) \sum_{a_1 \in \mathcal{A}}\pi_\theta(a_1|s_1)\sum_{s_2 \in \mathcal{S}} p(s_2 | s_1, a_1) \nabla_\theta V^{\pi_\theta}(s_2) ,
\]</div>
<br>
<p>규칙성이 잘 보일지 모르겠다. <span class="math notranslate nohighlight">\(\nabla_\theta V^{\pi_\theta}(s_t)\)</span>를 한번 전개 할때마다, 상태 <span class="math notranslate nohighlight">\(s_t\)</span>까지 도달할 확률을 곱해주고 각 상태 <span class="math notranslate nohighlight">\(a_t\)</span>에 대한 <span class="math notranslate nohighlight">\(\nabla_\theta \pi_\theta(a_t|s_t) Q^{\pi_\theta}(s_t, a_t)\)</span>와 <span class="math notranslate nohighlight">\(\nabla_\theta V^{\pi_\theta}(s_{t+1})\)</span> 항이 추가된다. 그리고 후자의 경우 다시 같은 원리로 전개할 수 있다. 이 전개 과정을 무한히 많이 수행한다고 하면  <span class="math notranslate nohighlight">\(\sum\limits_{a \in \mathcal{A}}\nabla_\theta \pi_\theta(a|s)Q^{\pi_\theta}(s, a)\)</span> 텀이 무한히 많아질 것이다. 이를 적어보면 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
\nabla_\theta J(\theta) = \sum_{t=0}^{\infty} \gamma^{t} \text{Pr}(s_t =s|\pi_\theta) \sum_{a \in \mathcal{A}} Q^{\pi_\theta}(s,a)\nabla_\theta \pi_\theta(a|s) ,
\]</div>
<br>
<p>여기서 <span class="math notranslate nohighlight">\(\text{Pr}(s_t=s | \pi_\theta)\)</span>은 정책 <span class="math notranslate nohighlight">\(\pi_\theta\)</span>를 따랐을 때 <span class="math notranslate nohighlight">\(t\)</span> 시점에서의 상태가 <span class="math notranslate nohighlight">\(s\)</span>일 확률이다. 위 식을 깔끔하게 기댓값 표현으로 나타내고 싶다. 만약 각 상태 <span class="math notranslate nohighlight">\(s\)</span>에서 다음과 같은 함수를 정의하면 probability distribution일까?</p>
<div class="math notranslate nohighlight">
\[
d_\pi(s):= \sum_{t=0}^{\infty} \gamma^t \text{Pr}(s_t=s | \pi_\theta),
\]</div>
<br>
<p>아쉽게도 아니다. 모든 <span class="math notranslate nohighlight">\(s\)</span> 에 대해서 <span class="math notranslate nohighlight">\(d_{\pi_\theta}(s)\)</span>를 더해보면 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
\sum_{s \in \mathcal{S}} \sum_{t=0}^{\infty} \gamma^t \text{Pr} (s_t = s | \pi_\theta) = 
\sum_{t=0}^{\infty} \gamma^t
\sum_{s \in \mathcal{S}}  \text{Pr} (s_t = s | \pi_\theta) = \sum_{t=0}^{\infty} \gamma^t =\frac{1}{1-\gamma}
\]</div>
<br>
<p>1이 되지 않는다. 그래서 위 함수를 보통 unnormalized discounted visited frequencies라고 부른다. 뭐 <span class="math notranslate nohighlight">\(\frac{1}{1-\gamma}\)</span>로 나눠 주면 probability distribution이 될 것이다. 그래서 확률 분포 <span class="math notranslate nohighlight">\(d_{\pi_\theta}\)</span> 를 다시 정의해주자 (사실 식의 생김새는 중요하지 않다.)</p>
<div class="math notranslate nohighlight">
\[
d_\pi(s):= (1-\gamma) \sum_{t=0}^{\infty} \gamma^t \text{Pr}(s_t=s | \pi_\theta),
\]</div>
<br>
<p>이를 사용하여 목적 함수의 그레디언트를 다시 적어주면 다음과 같다. 등호가 비례로 바뀌게 된다.</p>
<div class="math notranslate nohighlight">
\[
\nabla_\theta J(\theta) \propto \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s)  \sum_{a \in \mathcal{A}} Q^{\pi_\theta}(s,a)\nabla_\theta \pi_\theta(a|s) .
\]</div>
<br>
<p>여기서 로그 함수의 미분 공식과 합성 함수의 미분 공식을 사용하면 위 식을 더 깔끔하게 바꿀 수 있다. 우리는 <span class="math notranslate nohighlight">\((\log f(x))' = \frac{f'(x)}{f(x)}\)</span>을 사용할 것이다.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_{\theta} J(\theta) &amp; \propto &amp; \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s)  \sum_{a \in \mathcal{A}} Q^{\pi_\theta}(s,a)  \nabla_\theta \pi_\theta(a|s) \\
&amp; = &amp; \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s)  \sum_{a \in \mathcal{A}} Q^{\pi_\theta}(s,a) \pi_\theta(a|s) \frac{\nabla_\theta \pi_\theta(a|s)}{\pi_\theta(a|s)}  \\
&amp; = &amp; \sum_{s \in \mathcal{S}} d_{\pi_\theta}(s)  \sum_{a \in \mathcal{A}} \pi_\theta(a|s)  Q^{\pi_\theta}(s,a)\nabla_\theta \log \pi_\theta(a|s) \\
&amp; = &amp;\mathbb{E}_{\pi_\theta} \left[ Q^{\pi_\theta}(s,a)\nabla_\theta \log \pi_\theta(a|s) \right],
\end{align*}
\end{split}\]</div>
<br>
<p>이때, <span class="math notranslate nohighlight">\(\mathbb{E}_{\pi_\theta}\)</span>는 정책 <span class="math notranslate nohighlight">\(\pi_\theta\)</span>를 따랐을 때 얻게 되는 <span class="math notranslate nohighlight">\((s, a)\)</span>의 확률에 대한 기댓값을 의미한다. 이것으로 증명을 마친다.</p>
<script
   type="text/javascript"
   src="https://utteranc.es/client.js"
   async="async"
   repo="HiddenBeginner/Deep-Reinforcement-Learnings"
   issue-term="pathname"
   theme="github-light"
   label="💬 comment"
   crossorigin="anonymous"
/></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./book/Chapter2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../Chapter1/5-stochastic-approximation.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">5. </span>가치 함수 근사하기: Stochastic approximation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="2-reinforce.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span>REINFORCE</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 재야의 숨은 초보<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>